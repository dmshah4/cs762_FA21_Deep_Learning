{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q7-trial_2.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "9318a5e8588b49d8a834783684441fc5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_1242830c56a448659adf63aa0fa91b64",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_5b62acc626c44b769b954ddf437199dd",
              "IPY_MODEL_7a50eae1f66141c3bb80d617bd7ce188",
              "IPY_MODEL_8bdbcd1eb5574d60b9cce5e88ebe1bf8"
            ]
          }
        },
        "1242830c56a448659adf63aa0fa91b64": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5b62acc626c44b769b954ddf437199dd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_1efd8efed04e420991d844c0f28fbd54",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a5656c4f8a734c648c32e1ce0c9678a4"
          }
        },
        "7a50eae1f66141c3bb80d617bd7ce188": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_bc5108527e20489fbdbbeb4e00006631",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_e72fbc4eeca14838a2d3fdaba00439ba"
          }
        },
        "8bdbcd1eb5574d60b9cce5e88ebe1bf8": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_19515975aaad49d2822c5733d45dcbbc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 139MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_23196640cef14713be7359c9d130d2e6"
          }
        },
        "1efd8efed04e420991d844c0f28fbd54": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a5656c4f8a734c648c32e1ce0c9678a4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "bc5108527e20489fbdbbeb4e00006631": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "e72fbc4eeca14838a2d3fdaba00439ba": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "19515975aaad49d2822c5733d45dcbbc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "23196640cef14713be7359c9d130d2e6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6c2bd706860d4108b87faaf0bbc896ac": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_2a8fe688ef134d8693ce3a1d0aea09f4",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_b4894cdec5ef49348235fb0bea7d7fa9",
              "IPY_MODEL_88b2de81d1ca464face5a6c9a911dccd",
              "IPY_MODEL_5b69ff6eae6b49f8baeb161e7bcf1b3c"
            ]
          }
        },
        "2a8fe688ef134d8693ce3a1d0aea09f4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b4894cdec5ef49348235fb0bea7d7fa9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5c43fa83dd434f8b9b1b2da2f3de29e5",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_6e9c579a93ed40539e5ef2b1118af26e"
          }
        },
        "88b2de81d1ca464face5a6c9a911dccd": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_556f02dd9b2d4d0b942bd3ebe6f059dc",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 166618694,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 166618694,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_dd9fb9fe48a44e998f94163f898b0262"
          }
        },
        "5b69ff6eae6b49f8baeb161e7bcf1b3c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7dea253ea4a74f86a48b326a4207fd36",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 159M/159M [00:15&lt;00:00, 9.02MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f4616e4d39f44f3d80cd6fce243a1391"
          }
        },
        "5c43fa83dd434f8b9b1b2da2f3de29e5": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "6e9c579a93ed40539e5ef2b1118af26e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "556f02dd9b2d4d0b942bd3ebe6f059dc": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "dd9fb9fe48a44e998f94163f898b0262": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7dea253ea4a74f86a48b326a4207fd36": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f4616e4d39f44f3d80cd6fce243a1391": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zzzudyubDoT",
        "outputId": "7ce0349c-0656-49c2-efb5-fffc1c7537a2"
      },
      "source": [
        "# !pip install grad-cam\n",
        "!pip install ttach\n",
        "!pip install timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n",
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 35.4 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAN7HyEFDpR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "15757a93-29d6-473e-afe1-b7472c1314d8"
      },
      "source": [
        "!git clone https://github.com/vigneshuw/pytorch-grad-cam.git\n",
        "!pip install pytorch-grad-cam/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-grad-cam'...\n",
            "remote: Enumerating objects: 702, done.\u001b[K\n",
            "remote: Counting objects: 100% (241/241), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 702 (delta 139), reused 160 (delta 82), pack-reused 461\u001b[K\n",
            "Receiving objects: 100% (702/702), 1.90 MiB | 32.90 MiB/s, done.\n",
            "Resolving deltas: 100% (367/367), done.\n",
            "Processing ./pytorch-grad-cam\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (4.62.3)\n",
            "Requirement already satisfied: ttach>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (0.0.3)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (0.11.1+cu111)\n",
            "Collecting opencv-python>=4.5torch>=1.4\n",
            "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.3 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.5torch>=1.4->grad-cam==1.3.2) (1.19.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->grad-cam==1.3.2) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->grad-cam==1.3.2) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision>=0.5->grad-cam==1.3.2) (3.10.0.2)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.3.2-py3-none-any.whl size=20658 sha256=26e21813d8935d886853d21dfc5e1bdeb4c4b2696213ff3bf201f76aed780ab7\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/cb/7e/31a3553b353ebe761ab45b4267c016ff23c49ba5acd801a813\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: opencv-python, grad-cam\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed grad-cam-1.3.2 opencv-python-4.5.4.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgaoBy69mewq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b75059f1-4bb9-41c1-8d15-0e6e15ab37e6"
      },
      "source": [
        "# Google Collab Requirements / Checks\n",
        "from google.colab.patches import cv2_imshow # Change this before running elsewhere!!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4D2f1K8LyG9",
        "outputId": "a9541d75-4ebe-4370-dfc7-e34aca0e53b4"
      },
      "source": [
        "# cd into yolo directory\n",
        "%cd \"/content/gdrive/MyDrive/University of Wisconsin-Madison/2021 Fall/cs762/CS762_Deep_Learning_Project/yolov3\"\n",
        "# %cd \"/content/gdrive/MyDrive/CS762_Deep_Learning_Project/yolov3\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/137Ds-GMPEANJbJ-EgO6RczyM3GlmpY1n/CS762_Deep_Learning_Project/yolov3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhXncVRZOzw"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import darknet as dn\n",
        "from darknet_util import write_results\n",
        "\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtRudh4lZU-H"
      },
      "source": [
        "# Load manual ms coco dataset and bounding boxes\n",
        "# Varun's Directory\n",
        "dir_uri = '/content/gdrive/MyDrive/University of Wisconsin-Madison/2021 Fall/cs762'\n",
        "# Devesh's Directory\n",
        "# dir_uri = '/content/gdrive/MyDrive'\n",
        "\n",
        "proj_dir = dir_uri + '/CS762_Deep_Learning_Project'\n",
        "# df = pd.read_excel( proj_dir + '/fullBBOX.xlsx')\n",
        "\n",
        "dataset_folder = proj_dir + '/Q7/new_dataset' \n",
        "\n",
        "image_names = [\n",
        "          'person_backpack.jpg', 'person-no_backpack.jpg',\n",
        "          'person_car.jpeg', 'person-no_car.jpeg'\n",
        "          # 'dining table_chair.jpeg', 'dining table-no_chair.jpeg'\n",
        "]\n",
        "\n",
        "# Yolo COCO classes (80 only)\n",
        "def load_classes(namesfile):\n",
        "    fp = open(namesfile, \"r\")\n",
        "    names = fp.read().split(\"\\n\")[:-1]\n",
        "    return names\n",
        "\n",
        "yolo_classes = load_classes(\"data/coco.names\")\n",
        "\n",
        "# DETR COCO classes (91 - acutally 80 when N/A removed)\n",
        "DETR_CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "# Load iamge net categories\n",
        "imageNet_labels_path = dir_uri + \"/CS762_Deep_Learning_Project/datasets/imagenet_classes.txt\"\n",
        "# Read the categories\n",
        "with open(imageNet_labels_path, \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# Load bbox's\n",
        "# bbox_path = proj_dir + '/fullBBOX.xlsx'\n",
        "# bbox_df = pd.read_excel(bbox_path)\n",
        "# all_boxes = bbox_df.to_numpy()\n",
        "\n",
        "output_file = proj_dir + '/Q7/q7_trial2.csv'\n",
        "headers = ['image_name', 'threshold', 'yolo label', 'yolo accuracy', 'yolo whole image %', 'detr label', 'detr accuracy', 'detr whole image %']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZ-wNVcZW82"
      },
      "source": [
        "yolo_transform = transforms.Compose([\n",
        "    transforms.Resize((416, 416)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "detr_transform = transforms.Compose([\n",
        "    transforms.Resize(800),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# bbox given as one row from our fullBBox.xlsx file\n",
        "def eval_metric_percent_above(grayscale_cam, bbox, threshold):\n",
        "\n",
        "    # find area of bbox\n",
        "    topLeft_w = int(bbox[2])\n",
        "    topLeft_h = int(bbox[3])\n",
        "    botRight_w = int(bbox[6])\n",
        "    botRight_h = int(bbox[7])\n",
        "    width_bbox = abs(botRight_w - topLeft_w)\n",
        "    height_bbox = abs(topLeft_h - botRight_h)\n",
        "    area = width_bbox * height_bbox\n",
        "\n",
        "    # percent above threshold whole image\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "    count_above_threshold = above_threshold.sum()\n",
        "    total_count = grayscale_cam.shape[0] * grayscale_cam.shape[1]\n",
        "    percent_above_threshold_whole_image = (count_above_threshold / total_count) * 100\n",
        "\n",
        "    # percent above threshold in bbox\n",
        "    bbox_grayscale_cam = grayscale_cam[topLeft_h:botRight_h, topLeft_w:botRight_w]\n",
        "    bbox_above_threshold = (bbox_grayscale_cam > threshold)\n",
        "    bbox_count_above_threshold = bbox_above_threshold.sum()\n",
        "    bbox_total_count = bbox_grayscale_cam.shape[0] * bbox_grayscale_cam.shape[1]\n",
        "    percent_above_threshold_in_bbox = (bbox_count_above_threshold / bbox_total_count) * 100\n",
        "\n",
        "    # percent above threshold outside bbox\n",
        "    outside_bbox_cam = copy.deepcopy(grayscale_cam)\n",
        "    outside_bbox_cam[topLeft_h:botRight_h, topLeft_w:botRight_w] = np.zeros((height_bbox, width_bbox))\n",
        "    outside_bbox_above_threshold = (outside_bbox_cam > threshold)\n",
        "    outside_bbox_count_above_threshold = outside_bbox_above_threshold.sum()\n",
        "    outside_bbox_total_count = total_count - bbox_total_count\n",
        "    percent_above_threshold_outside_bbox = (outside_bbox_count_above_threshold / outside_bbox_total_count) * 100\n",
        "\n",
        "    # return\n",
        "    return percent_above_threshold_whole_image, percent_above_threshold_in_bbox, percent_above_threshold_outside_bbox\n",
        "\n",
        "def eval_metric_percent_above_simple(grayscale_cam, threshold):\n",
        "    # percent above threshold whole image\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "    count_above_threshold = above_threshold.sum()\n",
        "    total_count = grayscale_cam.shape[0] * grayscale_cam.shape[1]\n",
        "    percent_above_threshold_whole_image = (count_above_threshold / total_count) * 100\n",
        "\n",
        "    # return\n",
        "    return percent_above_threshold_whole_image\n",
        "\n",
        "def resizeBbox(orig_bbox, orig_img_size, new_img_size):\n",
        "    orig_h = orig_img_size[0]\n",
        "    orig_w = orig_img_size[1]\n",
        "    new_h = new_img_size[0]\n",
        "    new_w = new_img_size[1]\n",
        "\n",
        "    h_ratio = new_h / orig_h\n",
        "    w_ratio = new_w / orig_w\n",
        "\n",
        "    new_bbox = copy.deepcopy(orig_bbox)\n",
        "    new_bbox[2] = orig_bbox[2] * w_ratio\n",
        "    new_bbox[3] = orig_bbox[3] * h_ratio\n",
        "    new_bbox[4] = orig_bbox[4] * w_ratio\n",
        "    new_bbox[5] = orig_bbox[5] * h_ratio\n",
        "    new_bbox[6] = orig_bbox[6] * w_ratio\n",
        "    new_bbox[7] = orig_bbox[7] * h_ratio\n",
        "    new_bbox[8] = orig_bbox[8] * w_ratio\n",
        "    new_bbox[9] = orig_bbox[9] * h_ratio\n",
        "\n",
        "    return new_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIv2NsBy215r"
      },
      "source": [
        "yolo_transform_eval = transforms.Compose([\n",
        "    transforms.Resize((416, 416)),\n",
        "])\n",
        "\n",
        "detr_transform_eval = transforms.Compose([\n",
        "    transforms.Resize(800),\n",
        "])\n",
        "\n",
        "\n",
        "def show_cam_on_image_updated(img: np.ndarray,\n",
        "                      mask: np.ndarray,\n",
        "                      use_rgb: bool = False,\n",
        "                      colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
        "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
        "    By default the heatmap is in BGR format.\n",
        "    :param img: The base image in RGB or BGR format.\n",
        "    :param mask: The cam mask.\n",
        "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
        "    :param colormap: The OpenCV colormap to be used.\n",
        "    :returns: The default image with the cam overlay.\n",
        "    \"\"\"\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
        "    if use_rgb:\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "\n",
        "    if np.max(img) > 1:\n",
        "        raise Exception(\n",
        "            \"The input image should np.float32 in the range [0, 1]\")\n",
        "\n",
        "    cam = heatmap + img\n",
        "    cam = cam / np.max(cam)\n",
        "    return heatmap, np.uint8(255 * cam)\n",
        "\n",
        "\n",
        "def visualize_eval_metric(rgb_image, grayscale_cam, heatmap, bbox, threshold):\n",
        "    red_color = (0, 0, 255)\n",
        "    thickness = 2\n",
        "    start_point = (int(bbox[2]), int(bbox[3]))\n",
        "    end_point = (int(bbox[6]), int(bbox[7]))\n",
        "\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "\n",
        "    for i in range(grayscale_cam.shape[0]):\n",
        "      for j in range(grayscale_cam.shape[1]):\n",
        "        if(above_threshold[i, j] == False):\n",
        "          heatmap[i,j,0] = 0\n",
        "          heatmap[i,j,1] = 0\n",
        "          heatmap[i,j,2] = 0\n",
        "\n",
        "    cam = heatmap + rgb_image\n",
        "    cam = cam / np.max(cam)\n",
        "    cam = np.uint8(255 * cam)\n",
        "    cam = cv2.rectangle(cam, start_point, end_point, red_color, thickness)\n",
        "    return cam\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def detr_box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def detr_rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = detr_box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "# PLotting the results\n",
        "def get_detr_label_and_prob(pil_img, prob, boxes, target_class):\n",
        "    label = None\n",
        "    max_prob = 0\n",
        "\n",
        "    colors = COLORS * 100\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "        cl = p.argmax()\n",
        "        if (DETR_CLASSES[cl] == target_class):\n",
        "          if p[cl] > max_prob:\n",
        "            label = DETR_CLASSES[cl]\n",
        "            max_prob = p[cl]\n",
        "    \n",
        "    return label, max_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gye9G6bYcmPU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "9318a5e8588b49d8a834783684441fc5",
            "1242830c56a448659adf63aa0fa91b64",
            "5b62acc626c44b769b954ddf437199dd",
            "7a50eae1f66141c3bb80d617bd7ce188",
            "8bdbcd1eb5574d60b9cce5e88ebe1bf8",
            "1efd8efed04e420991d844c0f28fbd54",
            "a5656c4f8a734c648c32e1ce0c9678a4",
            "bc5108527e20489fbdbbeb4e00006631",
            "e72fbc4eeca14838a2d3fdaba00439ba",
            "19515975aaad49d2822c5733d45dcbbc",
            "23196640cef14713be7359c9d130d2e6",
            "6c2bd706860d4108b87faaf0bbc896ac",
            "2a8fe688ef134d8693ce3a1d0aea09f4",
            "b4894cdec5ef49348235fb0bea7d7fa9",
            "88b2de81d1ca464face5a6c9a911dccd",
            "5b69ff6eae6b49f8baeb161e7bcf1b3c",
            "5c43fa83dd434f8b9b1b2da2f3de29e5",
            "6e9c579a93ed40539e5ef2b1118af26e",
            "556f02dd9b2d4d0b942bd3ebe6f059dc",
            "dd9fb9fe48a44e998f94163f898b0262",
            "7dea253ea4a74f86a48b326a4207fd36",
            "f4616e4d39f44f3d80cd6fce243a1391"
          ]
        },
        "outputId": "af43e2ab-f656-44ae-e995-4bd94ec1698f"
      },
      "source": [
        "torch.set_grad_enabled(True)\n",
        "\n",
        "blocks = dn.parse_cfg(\"cfg/yolov3.cfg\")\n",
        "yolo_model = dn.create_modules(blocks)\n",
        "\n",
        "yolo_model = dn.Darknet(\"cfg/yolov3.cfg\", False)\n",
        "\n",
        "yolo_model.load_weights(\"weights/yolov3.weights\")\n",
        "yolo_model.eval();\n",
        "\n",
        "#### Adding the gradient function to the tensors\n",
        "for param in yolo_model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "detr_model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "detr_model.eval();\n",
        "\n",
        " #### Adding the gradient function to the tensors\n",
        "for param in detr_model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/detr/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9318a5e8588b49d8a834783684441fc5",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\" to /root/.cache/torch/hub/checkpoints/detr-r50-e632da11.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6c2bd706860d4108b87faaf0bbc896ac",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/159M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JLGhFsReLy5"
      },
      "source": [
        "yolo_target_layers = [yolo_model.module_list[77].conv_77]\n",
        "detr_target_layers = [detr_model.input_proj]\n",
        "\n",
        "target_category = None\n",
        "\n",
        "yolo_input_image_shape = (416, 416, 3)\n",
        "#detr_input_image_shape = (800, ___, 3)\n",
        "\n",
        "threshold_options = [0.7, 0.9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5Uzx3Mc_RM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "acad5042-e6d6-423e-f9a2-4303d9bbfac5"
      },
      "source": [
        "output_data = []\n",
        "\n",
        "for threshold in threshold_options:\n",
        "  for i in range(len( image_names ) ):\n",
        "    print(image_names[i])\n",
        "\n",
        "    # Create input\n",
        "    test_img = Image.open( dataset_folder + '/' + image_names[i] ).convert('RGB')\n",
        "    # orig_bbox = all_boxes[i]\n",
        "    # orig_img_shape = np.array(test_img).shape\n",
        "\n",
        "    # Create target lable\n",
        "    target_name = image_names[i].split( '-' )[0]\n",
        "    target_name = target_name.split( '_' )[0]\n",
        "\n",
        "    #\n",
        "    # YOLO\n",
        "    #\n",
        "    \n",
        "    # YOLO Target label\n",
        "    # handle mismatch spelling in yolo classes\n",
        "    yolo_target_name = target_name\n",
        "    if yolo_target_name == 'airplane':\n",
        "      yolo_target_name = 'aeroplane'\n",
        "    if yolo_target_name == 'dining table':\n",
        "      yolo_target_name = 'diningtable'\n",
        "      \n",
        "    target_category = [index for index, class_instance in enumerate(yolo_classes) if class_instance == yolo_target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = yolo_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Get output\n",
        "    with torch.no_grad():\n",
        "      output = yolo_model(input_batch)\n",
        "\n",
        "    yolo_predicted_label = ''\n",
        "    yolo_predicted_label_prob = 0\n",
        "\n",
        "    output = write_results(output[0], 0.5, 80)\n",
        "    # Find box with target class\n",
        "    if type(output) != int:\n",
        "      for box in output:\n",
        "        if target_category[0] == int(box[7]):\n",
        "          yolo_predicted_label_prob = 1 / (1 + math.exp(-float(box[6])))\n",
        "          yolo_predicted_label = target_name\n",
        "          break\n",
        "\n",
        "    if ( yolo_predicted_label == '' ):\n",
        "      print( \"YOLO did not get true classification for: {}\".format( image_names[i] ) )\n",
        "\n",
        "    # Grad cam time\n",
        "    yolo_cam = GradCAM(model=yolo_model, target_layers=yolo_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    yolo_grayscale_cam = yolo_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    yolo_grayscale_cam = yolo_grayscale_cam[0, :]\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    # new_bbox_yolo = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (yolo_input_image_shape[0], yolo_input_image_shape[1]))\n",
        "    # yolo_whole_img, yolo_in_bbox, yolo_out_bbox = eval_metric_percent_above(yolo_grayscale_cam, new_bbox_yolo, threshold)\n",
        "    yolo_whole_img = eval_metric_percent_above_simple( yolo_grayscale_cam, threshold )\n",
        "\n",
        "\n",
        "    #\n",
        "    # DETR\n",
        "    #\n",
        "\n",
        "    # DETR Target Label\n",
        "    target_category = [index for index, class_instance in enumerate(DETR_CLASSES) if class_instance == target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = detr_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Compute the predictions\n",
        "    with torch.no_grad():\n",
        "      output = detr_model(input_batch)\n",
        "\n",
        "    # and convert them into probabilities\n",
        "    # keep only predictions with 0.7+ confidence\n",
        "    probas = output['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "    keep = probas.max(-1).values > 0.9\n",
        "\n",
        "    # finally get the index of the prediction with highest score\n",
        "    detr_predicted_label = None\n",
        "    detr_predicted_label_prob = 0\n",
        "\n",
        "    # convert boxes from [0; 1] to image scales\n",
        "    bboxes_scaled = detr_rescale_bboxes(output['pred_boxes'][0, keep], test_img.size)\n",
        "\n",
        "    detr_predicted_label, detr_predicted_label_prob = get_detr_label_and_prob(test_img, probas[keep], bboxes_scaled, target_name)\n",
        "\n",
        "    # Grad cam time\n",
        "    detr_cam = GradCAM(model=detr_model, target_layers=detr_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    detr_grayscale_cam = detr_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    detr_grayscale_cam = detr_grayscale_cam[0, :]\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    # detr_input_image_shape = (detr_grayscale_cam.shape[0], detr_grayscale_cam.shape[1], 3)\n",
        "    # new_bbox_detr = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (detr_input_image_shape[0], detr_input_image_shape[1]))\n",
        "    # detr_whole_img, detr_in_bbox, detr_out_bbox = eval_metric_percent_above(detr_grayscale_cam, new_bbox_detr, threshold)\n",
        "    detr_whole_img = eval_metric_percent_above_simple( detr_grayscale_cam, threshold )\n",
        "\n",
        "    output_data.append( [image_names[i], threshold, yolo_predicted_label, yolo_predicted_label_prob, yolo_whole_img, detr_predicted_label, detr_predicted_label_prob, detr_whole_img] )\n",
        "\n",
        "    # Delete garbage for ram\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "person_backpack.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(7.3709, grad_fn=<AddBackward0>)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_detr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.9013, grad_fn=<AddBackward0>), tensor(6.8809, grad_fn=<AddBackward0>)]\n",
            "person-no_backpack.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.1430, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.6785, grad_fn=<AddBackward0>), tensor(9.9265, grad_fn=<AddBackward0>)]\n",
            "person_car.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(14.8408, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.1508, grad_fn=<AddBackward0>)]\n",
            "person-no_car.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.3184, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.4615, grad_fn=<AddBackward0>)]\n",
            "person_backpack.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(7.3709, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.9013, grad_fn=<AddBackward0>), tensor(6.8809, grad_fn=<AddBackward0>)]\n",
            "person-no_backpack.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.1430, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.6785, grad_fn=<AddBackward0>), tensor(9.9265, grad_fn=<AddBackward0>)]\n",
            "person_car.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(14.8408, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.1508, grad_fn=<AddBackward0>)]\n",
            "person-no_car.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.3184, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.4615, grad_fn=<AddBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDgBUlnOxPAb"
      },
      "source": [
        "df = pd.DataFrame (output_data, columns=headers)\n",
        "df.to_csv(output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1z840oV2gN1"
      },
      "source": [
        "output_images_dir = proj_dir + '/Q7/trial_2/'\n",
        "images_to_visualize = list( range( len( image_names ) ) )\n",
        "threshold = 0.7\n",
        "generate_vis_analysis = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtz3BiD2g9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "b8ef4a76-87e9-4875-e066-56b65517b817"
      },
      "source": [
        "if(generate_vis_analysis):\n",
        "  for i in images_to_visualize:\n",
        "    # Create input\n",
        "    test_img = Image.open( dataset_folder + '/' + image_names[i] ).convert('RGB')\n",
        "    yolo_path = output_images_dir + image_names[i] + '_yolo_whole'\n",
        "    yolo_path_t = output_images_dir + image_names[i] + '_yolo_thres'\n",
        "    detr_path = output_images_dir + image_names[i] + '_detr_whole'\n",
        "    detr_path_t = output_images_dir + image_names[i] + '_detr_thres'\n",
        "\n",
        "    # orig_bbox = all_boxes[i]\n",
        "    # orig_img_shape = np.array(test_img).shape\n",
        "\n",
        "    # Create target lable\n",
        "    target_name = image_names[i].split( '-' )[0]\n",
        "    target_name = target_name.split( '_' )[0]\n",
        "\n",
        "    #\n",
        "    # YOLO\n",
        "    #\n",
        "    \n",
        "    # YOLO Target label\n",
        "    # handle mismatch spelling in yolo classes\n",
        "    yolo_target_name = target_name\n",
        "    if yolo_target_name == 'airplane':\n",
        "      yolo_target_name = 'aeroplane'\n",
        "    if yolo_target_name == 'dining table':\n",
        "      yolo_target_name = 'diningtable'\n",
        "      \n",
        "    target_category = [index for index, class_instance in enumerate(yolo_classes) if class_instance == yolo_target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = yolo_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Grad cam time\n",
        "    yolo_cam = GradCAM(model=yolo_model, target_layers=yolo_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    yolo_grayscale_cam = yolo_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    yolo_grayscale_cam = yolo_grayscale_cam[0, :]\n",
        "\n",
        "    # process image to write gradcam on\n",
        "    rgb_image = np.array(yolo_transform_eval(test_img)).astype(np.float32)\n",
        "    rgb_image = rgb_image/255\n",
        "\n",
        "    # whole image gradcam visualization\n",
        "    yolo_heatmap, yolo_visualization = show_cam_on_image_updated(rgb_image, yolo_grayscale_cam, use_rgb=False)\n",
        "    cv2.imwrite(yolo_path, yolo_visualization)\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    #new_bbox_yolo = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (yolo_input_image_shape[0], yolo_input_image_shape[1]))\n",
        "    #yolo_visualized_cam_threshold = visualize_eval_metric(rgb_image, yolo_grayscale_cam, yolo_heatmap, new_bbox_yolo, threshold)\n",
        "    #cv2.imwrite(yolo_path_t, yolo_visualized_cam_threshold)\n",
        "\n",
        "\n",
        "    ######\n",
        "\n",
        "\n",
        "    #\n",
        "    # DETR\n",
        "    #\n",
        "\n",
        "    # DETR Target Label\n",
        "    target_category = [index for index, class_instance in enumerate(DETR_CLASSES) if class_instance == target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = detr_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Grad cam time\n",
        "    detr_cam = GradCAM(model=detr_model, target_layers=detr_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    detr_grayscale_cam = detr_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    detr_grayscale_cam = detr_grayscale_cam[0, :]\n",
        "\n",
        "    # process image to write gradcam on\n",
        "    rgb_image = np.array(detr_transform_eval(test_img)).astype(np.float32)\n",
        "    rgb_image = rgb_image/255\n",
        "\n",
        "    # whole image gradcam visualization\n",
        "    detr_heatmap, detr_visualization = show_cam_on_image_updated(rgb_image, detr_grayscale_cam, use_rgb=False)\n",
        "    cv2.imwrite(detr_path, detr_visualization)\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    #detr_input_image_shape = (detr_grayscale_cam.shape[0], detr_grayscale_cam.shape[1], 3)\n",
        "    #new_bbox_detr = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (detr_input_image_shape[0], detr_input_image_shape[1]))\n",
        "    #detr_visualized_cam_threshold = visualize_eval_metric(rgb_image, detr_grayscale_cam, detr_heatmap, new_bbox_detr, threshold)\n",
        "    #cv2.imwrite(detr_path_t, detr_visualized_cam_threshold)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "The max logit value is [tensor(7.3709, grad_fn=<AddBackward0>)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_detr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.9013, grad_fn=<AddBackward0>), tensor(6.8809, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.1430, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.6785, grad_fn=<AddBackward0>), tensor(9.9265, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(14.8408, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.1508, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.3184, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.4615, grad_fn=<AddBackward0>)]\n"
          ]
        }
      ]
    }
  ]
}