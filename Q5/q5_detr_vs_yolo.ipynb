{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q5-detr_vs_yolo.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "a3ef4e9734af4a62bd24e63aab70956c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_267463fe311d48debbf31cccd2aed8df",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_c366e3a973df4cfe96f3c7531eb9afda",
              "IPY_MODEL_21eaf41258b047288c05a7fff435feb1",
              "IPY_MODEL_5adceee6c84344da8c7386642b0d7bd0"
            ]
          }
        },
        "267463fe311d48debbf31cccd2aed8df": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "c366e3a973df4cfe96f3c7531eb9afda": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_444e0692336c4110a172b01fd092bb27",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_15d90dd6419741efa668b9322c91ceab"
          }
        },
        "21eaf41258b047288c05a7fff435feb1": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_97344a62ab4b4679ad1c93cac9885fbb",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_3e946e5231804862abd90491ba8995e5"
          }
        },
        "5adceee6c84344da8c7386642b0d7bd0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_7a96cf85c30343b58311f9a8eca55d14",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:01&lt;00:00, 71.3MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_8ef54c6e0ef5481fb2f5141fc65862d6"
          }
        },
        "444e0692336c4110a172b01fd092bb27": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "15d90dd6419741efa668b9322c91ceab": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "97344a62ab4b4679ad1c93cac9885fbb": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "3e946e5231804862abd90491ba8995e5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "7a96cf85c30343b58311f9a8eca55d14": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "8ef54c6e0ef5481fb2f5141fc65862d6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ddb90195924c424496d6c6a32e8c684d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_66a39b7c056c4d0b82f3b59ad4a4639e",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a87b93455fd54e50be702c403e4b0af7",
              "IPY_MODEL_e7f1be3678f24a40a74d5ef8dd9a9542",
              "IPY_MODEL_88fce764a1f845c3821e6ee8ebd2fe8d"
            ]
          }
        },
        "66a39b7c056c4d0b82f3b59ad4a4639e": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a87b93455fd54e50be702c403e4b0af7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_f2e0500f6929416faab219524592fdbe",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_a88728b6e86f40a4b1bd64e1ee353ce4"
          }
        },
        "e7f1be3678f24a40a74d5ef8dd9a9542": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_632373e46764443eb988a4572e83895c",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 166618694,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 166618694,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_128814bb037f489bbed419fea0837ca6"
          }
        },
        "88fce764a1f845c3821e6ee8ebd2fe8d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_5857016627fb4be68b745d5ede599d28",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 159M/159M [00:07&lt;00:00, 26.8MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_81a9b9c3b8554c00ba5a90f5fc1d096c"
          }
        },
        "f2e0500f6929416faab219524592fdbe": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "a88728b6e86f40a4b1bd64e1ee353ce4": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "632373e46764443eb988a4572e83895c": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "128814bb037f489bbed419fea0837ca6": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "5857016627fb4be68b745d5ede599d28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "81a9b9c3b8554c00ba5a90f5fc1d096c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zzzudyubDoT",
        "outputId": "3ce98d75-5955-46d5-b0af-ae2c40a587d9"
      },
      "source": [
        "# !pip install grad-cam\n",
        "!pip install ttach\n",
        "!pip install timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting ttach\n",
            "  Downloading ttach-0.0.3-py3-none-any.whl (9.8 kB)\n",
            "Installing collected packages: ttach\n",
            "Successfully installed ttach-0.0.3\n",
            "Collecting timm\n",
            "  Downloading timm-0.4.12-py3-none-any.whl (376 kB)\n",
            "\u001b[K     |████████████████████████████████| 376 kB 5.0 MB/s \n",
            "\u001b[?25hRequirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Installing collected packages: timm\n",
            "Successfully installed timm-0.4.12\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAN7HyEFDpR3",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e26c7c83-f402-46fc-f32f-21f4ea081e5b"
      },
      "source": [
        "!git clone https://github.com/vigneshuw/pytorch-grad-cam.git\n",
        "!pip install pytorch-grad-cam/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Cloning into 'pytorch-grad-cam'...\n",
            "remote: Enumerating objects: 702, done.\u001b[K\n",
            "remote: Counting objects: 100% (241/241), done.\u001b[K\n",
            "remote: Compressing objects: 100% (145/145), done.\u001b[K\n",
            "remote: Total 702 (delta 139), reused 160 (delta 82), pack-reused 461\u001b[K\n",
            "Receiving objects: 100% (702/702), 1.90 MiB | 13.96 MiB/s, done.\n",
            "Resolving deltas: 100% (367/367), done.\n",
            "Processing ./pytorch-grad-cam\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting opencv-python>=4.5torch>=1.4\n",
            "  Downloading opencv_python-4.5.4.60-cp37-cp37m-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (60.3 MB)\n",
            "\u001b[K     |████████████████████████████████| 60.3 MB 1.3 MB/s \n",
            "\u001b[?25hRequirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (4.62.3)\n",
            "Requirement already satisfied: ttach>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (0.0.3)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.5torch>=1.4->grad-cam==1.3.2) (1.19.5)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->grad-cam==1.3.2) (7.1.2)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->grad-cam==1.3.2) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision>=0.5->grad-cam==1.3.2) (3.10.0.2)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.3.2-py3-none-any.whl size=20658 sha256=3f72fa0e667ffac554fa617047f2c55e1b31e6060d6b5315b935b3c032252daf\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/cb/7e/31a3553b353ebe761ab45b4267c016ff23c49ba5acd801a813\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: opencv-python, grad-cam\n",
            "  Attempting uninstall: opencv-python\n",
            "    Found existing installation: opencv-python 4.1.2.30\n",
            "    Uninstalling opencv-python-4.1.2.30:\n",
            "      Successfully uninstalled opencv-python-4.1.2.30\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albumentations 0.1.12 requires imgaug<0.2.7,>=0.2.5, but you have imgaug 0.2.9 which is incompatible.\u001b[0m\n",
            "Successfully installed grad-cam-1.3.2 opencv-python-4.5.4.60\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HgaoBy69mewq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7b5790-40db-40c6-a362-a4b31f1543bd"
      },
      "source": [
        "# Google Collab Requirements / Checks\n",
        "from google.colab.patches import cv2_imshow # Change this before running elsewhere!!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "u4D2f1K8LyG9",
        "outputId": "b98beb91-a47f-4601-fe7c-8cf8f5ac4c22"
      },
      "source": [
        "# cd into yolo directory\n",
        "%cd \"/content/gdrive/MyDrive/University of Wisconsin-Madison/2021 Fall/cs762/CS762_Deep_Learning_Project/yolov3\"\n",
        "# %cd \"/content/gdrive/MyDrive/CS762_Deep_Learning_Project/yolov3\""
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "/content/gdrive/.shortcut-targets-by-id/137Ds-GMPEANJbJ-EgO6RczyM3GlmpY1n/CS762_Deep_Learning_Project/yolov3\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhXncVRZOzw"
      },
      "source": [
        "# Imports\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "import math\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torch.autograd import Variable\n",
        "from torchvision import transforms\n",
        "\n",
        "import darknet as dn\n",
        "from darknet_util import write_results\n",
        "\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image, preprocess_image"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtRudh4lZU-H"
      },
      "source": [
        "# Load manual ms coco dataset and bounding boxes\n",
        "# Varun's Directory\n",
        "dir_uri = '/content/gdrive/MyDrive/University of Wisconsin-Madison/2021 Fall/cs762'\n",
        "# Devesh's Directory\n",
        "# dir_uri = '/content/gdrive/MyDrive'\n",
        "\n",
        "proj_dir = dir_uri + '/CS762_Deep_Learning_Project'\n",
        "# df = pd.read_excel( proj_dir + '/fullBBOX.xlsx')\n",
        "\n",
        "dataset_folder = dir_uri + '/CS762_Deep_Learning_Project/datasets/Manual_Q5_Shuffled' \n",
        "image_names = []\n",
        "for filename in os.listdir(dataset_folder):\n",
        "  image_names.append(filename)\n",
        "\n",
        "image_names.sort(key=lambda x: 10 * int(x.split('-')[0]) + int(x.split('-')[2].split('.')[0]))\n",
        "\n",
        "# Yolo COCO classes (80 only)\n",
        "def load_classes(namesfile):\n",
        "    fp = open(namesfile, \"r\")\n",
        "    names = fp.read().split(\"\\n\")[:-1]\n",
        "    return names\n",
        "\n",
        "yolo_classes = load_classes(\"data/coco.names\")\n",
        "\n",
        "# DETR COCO classes (91 - acutally 80 when N/A removed)\n",
        "DETR_CLASSES = [\n",
        "    'N/A', 'person', 'bicycle', 'car', 'motorcycle', 'airplane', 'bus',\n",
        "    'train', 'truck', 'boat', 'traffic light', 'fire hydrant', 'N/A',\n",
        "    'stop sign', 'parking meter', 'bench', 'bird', 'cat', 'dog', 'horse',\n",
        "    'sheep', 'cow', 'elephant', 'bear', 'zebra', 'giraffe', 'N/A', 'backpack',\n",
        "    'umbrella', 'N/A', 'N/A', 'handbag', 'tie', 'suitcase', 'frisbee', 'skis',\n",
        "    'snowboard', 'sports ball', 'kite', 'baseball bat', 'baseball glove',\n",
        "    'skateboard', 'surfboard', 'tennis racket', 'bottle', 'N/A', 'wine glass',\n",
        "    'cup', 'fork', 'knife', 'spoon', 'bowl', 'banana', 'apple', 'sandwich',\n",
        "    'orange', 'broccoli', 'carrot', 'hot dog', 'pizza', 'donut', 'cake',\n",
        "    'chair', 'couch', 'potted plant', 'bed', 'N/A', 'dining table', 'N/A',\n",
        "    'N/A', 'toilet', 'N/A', 'tv', 'laptop', 'mouse', 'remote', 'keyboard',\n",
        "    'cell phone', 'microwave', 'oven', 'toaster', 'sink', 'refrigerator', 'N/A',\n",
        "    'book', 'clock', 'vase', 'scissors', 'teddy bear', 'hair drier',\n",
        "    'toothbrush'\n",
        "]\n",
        "\n",
        "# colors for visualization\n",
        "COLORS = [[0.000, 0.447, 0.741], [0.850, 0.325, 0.098], [0.929, 0.694, 0.125],\n",
        "          [0.494, 0.184, 0.556], [0.466, 0.674, 0.188], [0.301, 0.745, 0.933]]\n",
        "\n",
        "# Load iamge net categories\n",
        "imageNet_labels_path = dir_uri + \"/CS762_Deep_Learning_Project/datasets/imagenet_classes.txt\"\n",
        "# Read the categories\n",
        "with open(imageNet_labels_path, \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# Load bbox's\n",
        "# bbox_path = proj_dir + '/fullBBOX.xlsx'\n",
        "# bbox_df = pd.read_excel(bbox_path)\n",
        "# all_boxes = bbox_df.to_numpy()\n",
        "\n",
        "output_file = proj_dir + '/Q5/q5_output_detr_yolo.csv'\n",
        "headers = ['image_name', 'threshold', 'yolo label', 'yolo accuracy', 'yolo whole image %', 'detr label', 'detr accuracy', 'detr whole image %']"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZ-wNVcZW82"
      },
      "source": [
        "yolo_transform = transforms.Compose([\n",
        "    transforms.Resize((416, 416)),\n",
        "    transforms.ToTensor(),\n",
        "])\n",
        "\n",
        "detr_transform = transforms.Compose([\n",
        "    transforms.Resize(800),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize([0.485, 0.456, 0.406], [0.229, 0.224, 0.225])\n",
        "])\n",
        "\n",
        "\n",
        "# bbox given as one row from our fullBBox.xlsx file\n",
        "def eval_metric_percent_above(grayscale_cam, bbox, threshold):\n",
        "\n",
        "    # find area of bbox\n",
        "    topLeft_w = int(bbox[2])\n",
        "    topLeft_h = int(bbox[3])\n",
        "    botRight_w = int(bbox[6])\n",
        "    botRight_h = int(bbox[7])\n",
        "    width_bbox = abs(botRight_w - topLeft_w)\n",
        "    height_bbox = abs(topLeft_h - botRight_h)\n",
        "    area = width_bbox * height_bbox\n",
        "\n",
        "    # percent above threshold whole image\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "    count_above_threshold = above_threshold.sum()\n",
        "    total_count = grayscale_cam.shape[0] * grayscale_cam.shape[1]\n",
        "    percent_above_threshold_whole_image = (count_above_threshold / total_count) * 100\n",
        "\n",
        "    # percent above threshold in bbox\n",
        "    bbox_grayscale_cam = grayscale_cam[topLeft_h:botRight_h, topLeft_w:botRight_w]\n",
        "    bbox_above_threshold = (bbox_grayscale_cam > threshold)\n",
        "    bbox_count_above_threshold = bbox_above_threshold.sum()\n",
        "    bbox_total_count = bbox_grayscale_cam.shape[0] * bbox_grayscale_cam.shape[1]\n",
        "    percent_above_threshold_in_bbox = (bbox_count_above_threshold / bbox_total_count) * 100\n",
        "\n",
        "    # percent above threshold outside bbox\n",
        "    outside_bbox_cam = copy.deepcopy(grayscale_cam)\n",
        "    outside_bbox_cam[topLeft_h:botRight_h, topLeft_w:botRight_w] = np.zeros((height_bbox, width_bbox))\n",
        "    outside_bbox_above_threshold = (outside_bbox_cam > threshold)\n",
        "    outside_bbox_count_above_threshold = outside_bbox_above_threshold.sum()\n",
        "    outside_bbox_total_count = total_count - bbox_total_count\n",
        "    percent_above_threshold_outside_bbox = (outside_bbox_count_above_threshold / outside_bbox_total_count) * 100\n",
        "\n",
        "    # return\n",
        "    return percent_above_threshold_whole_image, percent_above_threshold_in_bbox, percent_above_threshold_outside_bbox\n",
        "\n",
        "def eval_metric_percent_above_simple(grayscale_cam, threshold):\n",
        "    # percent above threshold whole image\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "    count_above_threshold = above_threshold.sum()\n",
        "    total_count = grayscale_cam.shape[0] * grayscale_cam.shape[1]\n",
        "    percent_above_threshold_whole_image = (count_above_threshold / total_count) * 100\n",
        "\n",
        "    # return\n",
        "    return percent_above_threshold_whole_image\n",
        "\n",
        "def resizeBbox(orig_bbox, orig_img_size, new_img_size):\n",
        "    orig_h = orig_img_size[0]\n",
        "    orig_w = orig_img_size[1]\n",
        "    new_h = new_img_size[0]\n",
        "    new_w = new_img_size[1]\n",
        "\n",
        "    h_ratio = new_h / orig_h\n",
        "    w_ratio = new_w / orig_w\n",
        "\n",
        "    new_bbox = copy.deepcopy(orig_bbox)\n",
        "    new_bbox[2] = orig_bbox[2] * w_ratio\n",
        "    new_bbox[3] = orig_bbox[3] * h_ratio\n",
        "    new_bbox[4] = orig_bbox[4] * w_ratio\n",
        "    new_bbox[5] = orig_bbox[5] * h_ratio\n",
        "    new_bbox[6] = orig_bbox[6] * w_ratio\n",
        "    new_bbox[7] = orig_bbox[7] * h_ratio\n",
        "    new_bbox[8] = orig_bbox[8] * w_ratio\n",
        "    new_bbox[9] = orig_bbox[9] * h_ratio\n",
        "\n",
        "    return new_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IIv2NsBy215r"
      },
      "source": [
        "yolo_transform_eval = transforms.Compose([\n",
        "    transforms.Resize((416, 416)),\n",
        "])\n",
        "\n",
        "detr_transform_eval = transforms.Compose([\n",
        "    transforms.Resize(800),\n",
        "])\n",
        "\n",
        "\n",
        "def show_cam_on_image_updated(img: np.ndarray,\n",
        "                      mask: np.ndarray,\n",
        "                      use_rgb: bool = False,\n",
        "                      colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
        "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
        "    By default the heatmap is in BGR format.\n",
        "    :param img: The base image in RGB or BGR format.\n",
        "    :param mask: The cam mask.\n",
        "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
        "    :param colormap: The OpenCV colormap to be used.\n",
        "    :returns: The default image with the cam overlay.\n",
        "    \"\"\"\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
        "    if use_rgb:\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "\n",
        "    if np.max(img) > 1:\n",
        "        raise Exception(\n",
        "            \"The input image should np.float32 in the range [0, 1]\")\n",
        "\n",
        "    cam = heatmap + img\n",
        "    cam = cam / np.max(cam)\n",
        "    return heatmap, np.uint8(255 * cam)\n",
        "\n",
        "\n",
        "def visualize_eval_metric(rgb_image, grayscale_cam, heatmap, bbox, threshold):\n",
        "    red_color = (0, 0, 255)\n",
        "    thickness = 2\n",
        "    start_point = (int(bbox[2]), int(bbox[3]))\n",
        "    end_point = (int(bbox[6]), int(bbox[7]))\n",
        "\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "\n",
        "    for i in range(grayscale_cam.shape[0]):\n",
        "      for j in range(grayscale_cam.shape[1]):\n",
        "        if(above_threshold[i, j] == False):\n",
        "          heatmap[i,j,0] = 0\n",
        "          heatmap[i,j,1] = 0\n",
        "          heatmap[i,j,2] = 0\n",
        "\n",
        "    cam = heatmap + rgb_image\n",
        "    cam = cam / np.max(cam)\n",
        "    cam = np.uint8(255 * cam)\n",
        "    cam = cv2.rectangle(cam, start_point, end_point, red_color, thickness)\n",
        "    return cam\n",
        "\n",
        "# for output bounding box post-processing\n",
        "def detr_box_cxcywh_to_xyxy(x):\n",
        "    x_c, y_c, w, h = x.unbind(1)\n",
        "    b = [(x_c - 0.5 * w), (y_c - 0.5 * h),\n",
        "         (x_c + 0.5 * w), (y_c + 0.5 * h)]\n",
        "    return torch.stack(b, dim=1)\n",
        "\n",
        "def detr_rescale_bboxes(out_bbox, size):\n",
        "    img_w, img_h = size\n",
        "    b = detr_box_cxcywh_to_xyxy(out_bbox)\n",
        "    b = b * torch.tensor([img_w, img_h, img_w, img_h], dtype=torch.float32)\n",
        "    return b\n",
        "\n",
        "# PLotting the results\n",
        "def get_detr_label_and_prob(pil_img, prob, boxes, target_class):\n",
        "    label = None\n",
        "    max_prob = 0\n",
        "\n",
        "    colors = COLORS * 100\n",
        "    for p, (xmin, ymin, xmax, ymax), c in zip(prob, boxes.tolist(), colors):\n",
        "        cl = p.argmax()\n",
        "        if (DETR_CLASSES[cl] == target_class):\n",
        "          if p[cl] > max_prob:\n",
        "            label = DETR_CLASSES[cl]\n",
        "            max_prob = p[cl]\n",
        "    \n",
        "    return label, max_prob"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Gye9G6bYcmPU",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "a3ef4e9734af4a62bd24e63aab70956c",
            "267463fe311d48debbf31cccd2aed8df",
            "c366e3a973df4cfe96f3c7531eb9afda",
            "21eaf41258b047288c05a7fff435feb1",
            "5adceee6c84344da8c7386642b0d7bd0",
            "444e0692336c4110a172b01fd092bb27",
            "15d90dd6419741efa668b9322c91ceab",
            "97344a62ab4b4679ad1c93cac9885fbb",
            "3e946e5231804862abd90491ba8995e5",
            "7a96cf85c30343b58311f9a8eca55d14",
            "8ef54c6e0ef5481fb2f5141fc65862d6",
            "ddb90195924c424496d6c6a32e8c684d",
            "66a39b7c056c4d0b82f3b59ad4a4639e",
            "a87b93455fd54e50be702c403e4b0af7",
            "e7f1be3678f24a40a74d5ef8dd9a9542",
            "88fce764a1f845c3821e6ee8ebd2fe8d",
            "f2e0500f6929416faab219524592fdbe",
            "a88728b6e86f40a4b1bd64e1ee353ce4",
            "632373e46764443eb988a4572e83895c",
            "128814bb037f489bbed419fea0837ca6",
            "5857016627fb4be68b745d5ede599d28",
            "81a9b9c3b8554c00ba5a90f5fc1d096c"
          ]
        },
        "outputId": "3806cff3-37a9-41f5-bafa-520956d92db9"
      },
      "source": [
        "torch.set_grad_enabled(True)\n",
        "\n",
        "blocks = dn.parse_cfg(\"cfg/yolov3.cfg\")\n",
        "yolo_model = dn.create_modules(blocks)\n",
        "\n",
        "yolo_model = dn.Darknet(\"cfg/yolov3.cfg\", False)\n",
        "\n",
        "yolo_model.load_weights(\"weights/yolov3.weights\")\n",
        "yolo_model.eval();\n",
        "\n",
        "#### Adding the gradient function to the tensors\n",
        "for param in yolo_model.parameters():\n",
        "    param.requires_grad = True\n",
        "\n",
        "detr_model = torch.hub.load('facebookresearch/detr', 'detr_resnet50', pretrained=True)\n",
        "detr_model.eval();\n",
        "\n",
        " #### Adding the gradient function to the tensors\n",
        "for param in detr_model.parameters():\n",
        "    param.requires_grad = True"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/detr/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a3ef4e9734af4a62bd24e63aab70956c",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://dl.fbaipublicfiles.com/detr/detr-r50-e632da11.pth\" to /root/.cache/torch/hub/checkpoints/detr-r50-e632da11.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ddb90195924c424496d6c6a32e8c684d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/159M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JLGhFsReLy5"
      },
      "source": [
        "yolo_target_layers = [yolo_model.module_list[77].conv_77]\n",
        "detr_target_layers = [detr_model.input_proj]\n",
        "\n",
        "target_category = None\n",
        "\n",
        "yolo_input_image_shape = (416, 416, 3)\n",
        "#detr_input_image_shape = (800, ___, 3)\n",
        "\n",
        "threshold_options = [0.7, 0.9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5Uzx3Mc_RM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4ff43920-e466-46c0-9580-0297e90f5078"
      },
      "source": [
        "output_data = []\n",
        "\n",
        "for threshold in threshold_options:\n",
        "  for i in range(len( image_names ) ):\n",
        "    print(image_names[i])\n",
        "\n",
        "    # Create input\n",
        "    test_img = Image.open( dataset_folder + '/' + image_names[i] ).convert('RGB')\n",
        "    # orig_bbox = all_boxes[i]\n",
        "    # orig_img_shape = np.array(test_img).shape\n",
        "\n",
        "    # Create target lable\n",
        "    target_name = image_names[i].split( '-' )[1]\n",
        "    target_name = target_name.split( '.' )[0]\n",
        "\n",
        "    #\n",
        "    # YOLO\n",
        "    #\n",
        "    \n",
        "    # YOLO Target label\n",
        "    # handle mismatch spelling in yolo classes\n",
        "    yolo_target_name = target_name\n",
        "    if yolo_target_name == 'airplane':\n",
        "      yolo_target_name = 'aeroplane'\n",
        "      \n",
        "    target_category = [index for index, class_instance in enumerate(yolo_classes) if class_instance == yolo_target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = yolo_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Get output\n",
        "    with torch.no_grad():\n",
        "      output = yolo_model(input_batch)\n",
        "\n",
        "    yolo_predicted_label = ''\n",
        "    yolo_predicted_label_prob = 0\n",
        "\n",
        "    output = write_results(output[0], 0.5, 80)\n",
        "    # Find box with target class\n",
        "    if type(output) != int:\n",
        "      for box in output:\n",
        "        if target_category[0] == int(box[7]):\n",
        "          yolo_predicted_label_prob = 1 / (1 + math.exp(-float(box[6])))\n",
        "          yolo_predicted_label = target_name\n",
        "          break\n",
        "\n",
        "    if ( yolo_predicted_label == '' ):\n",
        "      print( \"YOLO did not get true classification for: {}\".format( image_names[i] ) )\n",
        "\n",
        "    # Grad cam time\n",
        "    yolo_cam = GradCAM(model=yolo_model, target_layers=yolo_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    yolo_grayscale_cam = yolo_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    yolo_grayscale_cam = yolo_grayscale_cam[0, :]\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    # new_bbox_yolo = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (yolo_input_image_shape[0], yolo_input_image_shape[1]))\n",
        "    # yolo_whole_img, yolo_in_bbox, yolo_out_bbox = eval_metric_percent_above(yolo_grayscale_cam, new_bbox_yolo, threshold)\n",
        "    yolo_whole_img = eval_metric_percent_above_simple( yolo_grayscale_cam, threshold )\n",
        "\n",
        "\n",
        "    #\n",
        "    # DETR\n",
        "    #\n",
        "\n",
        "    # DETR Target Label\n",
        "    target_category = [index for index, class_instance in enumerate(DETR_CLASSES) if class_instance == target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = detr_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Compute the predictions\n",
        "    with torch.no_grad():\n",
        "      output = detr_model(input_batch)\n",
        "\n",
        "    # and convert them into probabilities\n",
        "    # keep only predictions with 0.7+ confidence\n",
        "    probas = output['pred_logits'].softmax(-1)[0, :, :-1]\n",
        "    keep = probas.max(-1).values > 0.9\n",
        "\n",
        "    # finally get the index of the prediction with highest score\n",
        "    detr_predicted_label = None\n",
        "    detr_predicted_label_prob = 0\n",
        "\n",
        "    # convert boxes from [0; 1] to image scales\n",
        "    bboxes_scaled = detr_rescale_bboxes(output['pred_boxes'][0, keep], test_img.size)\n",
        "\n",
        "    detr_predicted_label, detr_predicted_label_prob = get_detr_label_and_prob(test_img, probas[keep], bboxes_scaled, target_name)\n",
        "\n",
        "    # Grad cam time\n",
        "    detr_cam = GradCAM(model=detr_model, target_layers=detr_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    detr_grayscale_cam = detr_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    detr_grayscale_cam = detr_grayscale_cam[0, :]\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    # detr_input_image_shape = (detr_grayscale_cam.shape[0], detr_grayscale_cam.shape[1], 3)\n",
        "    # new_bbox_detr = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (detr_input_image_shape[0], detr_input_image_shape[1]))\n",
        "    # detr_whole_img, detr_in_bbox, detr_out_bbox = eval_metric_percent_above(detr_grayscale_cam, new_bbox_detr, threshold)\n",
        "    detr_whole_img = eval_metric_percent_above_simple( detr_grayscale_cam, threshold )\n",
        "\n",
        "    output_data.append( [image_names[i], threshold, yolo_predicted_label, yolo_predicted_label_prob, yolo_whole_img, detr_predicted_label, detr_predicted_label_prob, detr_whole_img] )\n",
        "\n",
        "    # Delete garbage for ram\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-person-0.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.8777, grad_fn=<AddBackward0>)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_detr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.2641, grad_fn=<AddBackward0>)]\n",
            "0-person-1.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.2817, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.0671, grad_fn=<AddBackward0>), tensor(10.0423, grad_fn=<AddBackward0>), tensor(8.2343, grad_fn=<AddBackward0>), tensor(8.4077, grad_fn=<AddBackward0>), tensor(11.2500, grad_fn=<AddBackward0>), tensor(8.8875, grad_fn=<AddBackward0>)]\n",
            "0-person-2.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.7079, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.0867, grad_fn=<AddBackward0>), tensor(10.6647, grad_fn=<AddBackward0>), tensor(9.4661, grad_fn=<AddBackward0>), tensor(8.3907, grad_fn=<AddBackward0>), tensor(11.3320, grad_fn=<AddBackward0>), tensor(7.9635, grad_fn=<AddBackward0>), tensor(10.3141, grad_fn=<AddBackward0>)]\n",
            "0-person-3.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(10.3187, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.1791, grad_fn=<AddBackward0>), tensor(13.1549, grad_fn=<AddBackward0>)]\n",
            "1-person-0.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.7985, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.4400, grad_fn=<AddBackward0>), tensor(8.5124, grad_fn=<AddBackward0>)]\n",
            "1-person-1.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.5693, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.2904, grad_fn=<AddBackward0>), tensor(6.4458, grad_fn=<AddBackward0>), tensor(9.4051, grad_fn=<AddBackward0>), tensor(6.3720, grad_fn=<AddBackward0>), tensor(7.9595, grad_fn=<AddBackward0>), tensor(10.3441, grad_fn=<AddBackward0>), tensor(7.6483, grad_fn=<AddBackward0>), tensor(6.8112, grad_fn=<AddBackward0>), tensor(9.3623, grad_fn=<AddBackward0>)]\n",
            "1-person-2.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(10.5806, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.1664, grad_fn=<AddBackward0>), tensor(5.9601, grad_fn=<AddBackward0>), tensor(6.9254, grad_fn=<AddBackward0>), tensor(8.0281, grad_fn=<AddBackward0>), tensor(6.0639, grad_fn=<AddBackward0>), tensor(8.3996, grad_fn=<AddBackward0>), tensor(7.6437, grad_fn=<AddBackward0>), tensor(6.5000, grad_fn=<AddBackward0>), tensor(8.7214, grad_fn=<AddBackward0>), tensor(7.4593, grad_fn=<AddBackward0>), tensor(8.0720, grad_fn=<AddBackward0>), tensor(8.2851, grad_fn=<AddBackward0>), tensor(6.7956, grad_fn=<AddBackward0>)]\n",
            "1-person-3.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.8977, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(7.9916, grad_fn=<AddBackward0>), tensor(12.2227, grad_fn=<AddBackward0>), tensor(8.8902, grad_fn=<AddBackward0>)]\n",
            "2-cat-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.9004, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.0008, grad_fn=<AddBackward0>)]\n",
            "2-cat-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.6022, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.2536, grad_fn=<AddBackward0>), tensor(10.2597, grad_fn=<AddBackward0>), tensor(10.8414, grad_fn=<AddBackward0>), tensor(8.4203, grad_fn=<AddBackward0>), tensor(10.9414, grad_fn=<AddBackward0>), tensor(10.3100, grad_fn=<AddBackward0>)]\n",
            "2-cat-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.2895, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.2339, grad_fn=<AddBackward0>), tensor(9.7704, grad_fn=<AddBackward0>), tensor(10.7959, grad_fn=<AddBackward0>), tensor(9.3671, grad_fn=<AddBackward0>)]\n",
            "2-cat-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.5915, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.8617, grad_fn=<AddBackward0>), tensor(12.9043, grad_fn=<AddBackward0>)]\n",
            "3-cat-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.8770, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.0523, grad_fn=<AddBackward0>)]\n",
            "3-cat-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.7283, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.3215, grad_fn=<AddBackward0>), tensor(10.6083, grad_fn=<AddBackward0>), tensor(10.2522, grad_fn=<AddBackward0>)]\n",
            "3-cat-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.1183, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(7.8299, grad_fn=<AddBackward0>), tensor(10.7542, grad_fn=<AddBackward0>), tensor(12.0724, grad_fn=<AddBackward0>)]\n",
            "3-cat-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.3833, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.3926, grad_fn=<AddBackward0>), tensor(13.0534, grad_fn=<AddBackward0>)]\n",
            "4-dog-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(7.8346, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.6772, grad_fn=<AddBackward0>)]\n",
            "4-dog-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.6307, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.7510, grad_fn=<AddBackward0>), tensor(10.6105, grad_fn=<AddBackward0>), tensor(8.9222, grad_fn=<AddBackward0>), tensor(10.3857, grad_fn=<AddBackward0>), tensor(8.9800, grad_fn=<AddBackward0>)]\n",
            "4-dog-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.3451, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.7194, grad_fn=<AddBackward0>), tensor(9.7172, grad_fn=<AddBackward0>), tensor(9.3925, grad_fn=<AddBackward0>), tensor(7.9633, grad_fn=<AddBackward0>), tensor(10.6478, grad_fn=<AddBackward0>), tensor(10.4517, grad_fn=<AddBackward0>)]\n",
            "4-dog-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.1242, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.2688, grad_fn=<AddBackward0>), tensor(10.9516, grad_fn=<AddBackward0>), tensor(11.3875, grad_fn=<AddBackward0>)]\n",
            "5-dog-0.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.8626, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.7770, grad_fn=<AddBackward0>)]\n",
            "5-dog-1.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.2652, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.7418, grad_fn=<AddBackward0>), tensor(9.2782, grad_fn=<AddBackward0>)]\n",
            "5-dog-2.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.3589, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.3027, grad_fn=<AddBackward0>), tensor(9.9220, grad_fn=<AddBackward0>), tensor(7.1019, grad_fn=<AddBackward0>)]\n",
            "5-dog-3.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.4498, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.6276, grad_fn=<AddBackward0>), tensor(12.1775, grad_fn=<AddBackward0>)]\n",
            "6-bird-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.6841, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.4713, grad_fn=<AddBackward0>)]\n",
            "6-bird-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.2599, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.6543, grad_fn=<AddBackward0>), tensor(11.9787, grad_fn=<AddBackward0>), tensor(12.5092, grad_fn=<AddBackward0>)]\n",
            "6-bird-2.jpeg\n",
            "YOLO did not get true classification for: 6-bird-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-2.6965, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.0165, grad_fn=<AddBackward0>), tensor(11.5635, grad_fn=<AddBackward0>), tensor(10.8983, grad_fn=<AddBackward0>), tensor(9.7250, grad_fn=<AddBackward0>)]\n",
            "6-bird-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.7369, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.7170, grad_fn=<AddBackward0>)]\n",
            "7-bird-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.6064, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.6879, grad_fn=<AddBackward0>)]\n",
            "7-bird-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-2.8543, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.6150, grad_fn=<AddBackward0>), tensor(12.4657, grad_fn=<AddBackward0>)]\n",
            "7-bird-2.jpeg\n",
            "YOLO did not get true classification for: 7-bird-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-7.7046, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.8684, grad_fn=<AddBackward0>), tensor(11.7103, grad_fn=<AddBackward0>), tensor(11.2662, grad_fn=<AddBackward0>)]\n",
            "7-bird-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.9465, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.1772, grad_fn=<AddBackward0>), tensor(12.4448, grad_fn=<AddBackward0>)]\n",
            "0-person-0.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.8777, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.2641, grad_fn=<AddBackward0>)]\n",
            "0-person-1.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.2817, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.0671, grad_fn=<AddBackward0>), tensor(10.0423, grad_fn=<AddBackward0>), tensor(8.2343, grad_fn=<AddBackward0>), tensor(8.4077, grad_fn=<AddBackward0>), tensor(11.2500, grad_fn=<AddBackward0>), tensor(8.8875, grad_fn=<AddBackward0>)]\n",
            "0-person-2.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.7079, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.0867, grad_fn=<AddBackward0>), tensor(10.6647, grad_fn=<AddBackward0>), tensor(9.4661, grad_fn=<AddBackward0>), tensor(8.3907, grad_fn=<AddBackward0>), tensor(11.3320, grad_fn=<AddBackward0>), tensor(7.9635, grad_fn=<AddBackward0>), tensor(10.3141, grad_fn=<AddBackward0>)]\n",
            "0-person-3.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(10.3187, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.1791, grad_fn=<AddBackward0>), tensor(13.1549, grad_fn=<AddBackward0>)]\n",
            "1-person-0.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.7985, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.4400, grad_fn=<AddBackward0>), tensor(8.5124, grad_fn=<AddBackward0>)]\n",
            "1-person-1.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.5693, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.2904, grad_fn=<AddBackward0>), tensor(6.4458, grad_fn=<AddBackward0>), tensor(9.4051, grad_fn=<AddBackward0>), tensor(6.3720, grad_fn=<AddBackward0>), tensor(7.9595, grad_fn=<AddBackward0>), tensor(10.3441, grad_fn=<AddBackward0>), tensor(7.6483, grad_fn=<AddBackward0>), tensor(6.8112, grad_fn=<AddBackward0>), tensor(9.3623, grad_fn=<AddBackward0>)]\n",
            "1-person-2.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(10.5806, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.1664, grad_fn=<AddBackward0>), tensor(5.9601, grad_fn=<AddBackward0>), tensor(6.9254, grad_fn=<AddBackward0>), tensor(8.0281, grad_fn=<AddBackward0>), tensor(6.0639, grad_fn=<AddBackward0>), tensor(8.3996, grad_fn=<AddBackward0>), tensor(7.6437, grad_fn=<AddBackward0>), tensor(6.5000, grad_fn=<AddBackward0>), tensor(8.7214, grad_fn=<AddBackward0>), tensor(7.4593, grad_fn=<AddBackward0>), tensor(8.0720, grad_fn=<AddBackward0>), tensor(8.2851, grad_fn=<AddBackward0>), tensor(6.7956, grad_fn=<AddBackward0>)]\n",
            "1-person-3.jpg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.8977, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(7.9916, grad_fn=<AddBackward0>), tensor(12.2227, grad_fn=<AddBackward0>), tensor(8.8902, grad_fn=<AddBackward0>)]\n",
            "2-cat-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.9004, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.0008, grad_fn=<AddBackward0>)]\n",
            "2-cat-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.6022, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.2536, grad_fn=<AddBackward0>), tensor(10.2597, grad_fn=<AddBackward0>), tensor(10.8414, grad_fn=<AddBackward0>), tensor(8.4203, grad_fn=<AddBackward0>), tensor(10.9414, grad_fn=<AddBackward0>), tensor(10.3100, grad_fn=<AddBackward0>)]\n",
            "2-cat-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.2895, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.2339, grad_fn=<AddBackward0>), tensor(9.7704, grad_fn=<AddBackward0>), tensor(10.7959, grad_fn=<AddBackward0>), tensor(9.3671, grad_fn=<AddBackward0>)]\n",
            "2-cat-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.5915, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.8617, grad_fn=<AddBackward0>), tensor(12.9043, grad_fn=<AddBackward0>)]\n",
            "3-cat-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.8770, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.0523, grad_fn=<AddBackward0>)]\n",
            "3-cat-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.7283, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.3215, grad_fn=<AddBackward0>), tensor(10.6083, grad_fn=<AddBackward0>), tensor(10.2522, grad_fn=<AddBackward0>)]\n",
            "3-cat-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.1183, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(7.8299, grad_fn=<AddBackward0>), tensor(10.7542, grad_fn=<AddBackward0>), tensor(12.0724, grad_fn=<AddBackward0>)]\n",
            "3-cat-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.3833, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.3926, grad_fn=<AddBackward0>), tensor(13.0534, grad_fn=<AddBackward0>)]\n",
            "4-dog-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(7.8346, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.6772, grad_fn=<AddBackward0>)]\n",
            "4-dog-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.6307, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.7510, grad_fn=<AddBackward0>), tensor(10.6105, grad_fn=<AddBackward0>), tensor(8.9222, grad_fn=<AddBackward0>), tensor(10.3857, grad_fn=<AddBackward0>), tensor(8.9800, grad_fn=<AddBackward0>)]\n",
            "4-dog-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.3451, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.7194, grad_fn=<AddBackward0>), tensor(9.7172, grad_fn=<AddBackward0>), tensor(9.3925, grad_fn=<AddBackward0>), tensor(7.9633, grad_fn=<AddBackward0>), tensor(10.6478, grad_fn=<AddBackward0>), tensor(10.4517, grad_fn=<AddBackward0>)]\n",
            "4-dog-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.1242, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.2688, grad_fn=<AddBackward0>), tensor(10.9516, grad_fn=<AddBackward0>), tensor(11.3875, grad_fn=<AddBackward0>)]\n",
            "5-dog-0.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.8626, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.7770, grad_fn=<AddBackward0>)]\n",
            "5-dog-1.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.2652, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.7418, grad_fn=<AddBackward0>), tensor(9.2782, grad_fn=<AddBackward0>)]\n",
            "5-dog-2.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.3589, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.3027, grad_fn=<AddBackward0>), tensor(9.9220, grad_fn=<AddBackward0>), tensor(7.1019, grad_fn=<AddBackward0>)]\n",
            "5-dog-3.png\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.4498, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.6276, grad_fn=<AddBackward0>), tensor(12.1775, grad_fn=<AddBackward0>)]\n",
            "6-bird-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.6841, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.4713, grad_fn=<AddBackward0>)]\n",
            "6-bird-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.2599, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.6543, grad_fn=<AddBackward0>), tensor(11.9787, grad_fn=<AddBackward0>), tensor(12.5092, grad_fn=<AddBackward0>)]\n",
            "6-bird-2.jpeg\n",
            "YOLO did not get true classification for: 6-bird-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-2.6965, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.0165, grad_fn=<AddBackward0>), tensor(11.5635, grad_fn=<AddBackward0>), tensor(10.8983, grad_fn=<AddBackward0>), tensor(9.7250, grad_fn=<AddBackward0>)]\n",
            "6-bird-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.7369, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.7170, grad_fn=<AddBackward0>)]\n",
            "7-bird-0.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.6064, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.6879, grad_fn=<AddBackward0>)]\n",
            "7-bird-1.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-2.8543, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.6150, grad_fn=<AddBackward0>), tensor(12.4657, grad_fn=<AddBackward0>)]\n",
            "7-bird-2.jpeg\n",
            "YOLO did not get true classification for: 7-bird-2.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-7.7046, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.8684, grad_fn=<AddBackward0>), tensor(11.7103, grad_fn=<AddBackward0>), tensor(11.2662, grad_fn=<AddBackward0>)]\n",
            "7-bird-3.jpeg\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.9465, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.1772, grad_fn=<AddBackward0>), tensor(12.4448, grad_fn=<AddBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDgBUlnOxPAb"
      },
      "source": [
        "df = pd.DataFrame (output_data, columns=headers)\n",
        "df.to_csv(output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e1z840oV2gN1"
      },
      "source": [
        "output_images_dir = proj_dir + '/Q5/output_images/'\n",
        "images_to_visualize = list( range( len( image_names ) ) )\n",
        "threshold = 0.7\n",
        "generate_vis_analysis = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "uBtz3BiD2g9i",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4b6e1886-f325-463c-cd86-cbc545e3fba0"
      },
      "source": [
        "if(generate_vis_analysis):\n",
        "  for i in images_to_visualize:\n",
        "    # Create input\n",
        "    test_img = Image.open( dataset_folder + '/' + image_names[i] ).convert('RGB')\n",
        "    yolo_path = output_images_dir + image_names[i] + '_yolo_whole'\n",
        "    yolo_path_t = output_images_dir + image_names[i] + '_yolo_thres'\n",
        "    detr_path = output_images_dir + image_names[i] + '_detr_whole'\n",
        "    detr_path_t = output_images_dir + image_names[i] + '_detr_thres'\n",
        "\n",
        "    # orig_bbox = all_boxes[i]\n",
        "    # orig_img_shape = np.array(test_img).shape\n",
        "\n",
        "    # Create target lable\n",
        "    target_name = image_names[i].split( '-' )[1]\n",
        "    target_name = target_name.split( '.' )[0]\n",
        "\n",
        "    #\n",
        "    # YOLO\n",
        "    #\n",
        "    \n",
        "    # YOLO Target label\n",
        "    # handle mismatch spelling in yolo classes\n",
        "    yolo_target_name = target_name\n",
        "    if yolo_target_name == 'airplane':\n",
        "      yolo_target_name = 'aeroplane'\n",
        "      \n",
        "    target_category = [index for index, class_instance in enumerate(yolo_classes) if class_instance == yolo_target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = yolo_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Grad cam time\n",
        "    yolo_cam = GradCAM(model=yolo_model, target_layers=yolo_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    yolo_grayscale_cam = yolo_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    yolo_grayscale_cam = yolo_grayscale_cam[0, :]\n",
        "\n",
        "    # process image to write gradcam on\n",
        "    rgb_image = np.array(yolo_transform_eval(test_img)).astype(np.float32)\n",
        "    rgb_image = rgb_image/255\n",
        "\n",
        "    # whole image gradcam visualization\n",
        "    yolo_heatmap, yolo_visualization = show_cam_on_image_updated(rgb_image, yolo_grayscale_cam, use_rgb=False)\n",
        "    cv2.imwrite(yolo_path, yolo_visualization)\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    #new_bbox_yolo = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (yolo_input_image_shape[0], yolo_input_image_shape[1]))\n",
        "    #yolo_visualized_cam_threshold = visualize_eval_metric(rgb_image, yolo_grayscale_cam, yolo_heatmap, new_bbox_yolo, threshold)\n",
        "    #cv2.imwrite(yolo_path_t, yolo_visualized_cam_threshold)\n",
        "\n",
        "\n",
        "    ######\n",
        "\n",
        "\n",
        "    #\n",
        "    # DETR\n",
        "    #\n",
        "\n",
        "    # DETR Target Label\n",
        "    target_category = [index for index, class_instance in enumerate(DETR_CLASSES) if class_instance == target_name]\n",
        "\n",
        "    # Input\n",
        "    input_tensor = detr_transform(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "\n",
        "    # Grad cam time\n",
        "    detr_cam = GradCAM(model=detr_model, target_layers=detr_target_layers, use_cuda=False, reshape_transform=None, classification_logit=\"pred_logits\")\n",
        "    detr_grayscale_cam = detr_cam(input_tensor=input_batch, target_category=target_category, eigen_smooth=False, aug_smooth=False)\n",
        "    detr_grayscale_cam = detr_grayscale_cam[0, :]\n",
        "\n",
        "    # process image to write gradcam on\n",
        "    rgb_image = np.array(detr_transform_eval(test_img)).astype(np.float32)\n",
        "    rgb_image = rgb_image/255\n",
        "\n",
        "    # whole image gradcam visualization\n",
        "    detr_heatmap, detr_visualization = show_cam_on_image_updated(rgb_image, detr_grayscale_cam, use_rgb=False)\n",
        "    cv2.imwrite(detr_path, detr_visualization)\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    #detr_input_image_shape = (detr_grayscale_cam.shape[0], detr_grayscale_cam.shape[1], 3)\n",
        "    #new_bbox_detr = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (detr_input_image_shape[0], detr_input_image_shape[1]))\n",
        "    #detr_visualized_cam_threshold = visualize_eval_metric(rgb_image, detr_grayscale_cam, detr_heatmap, new_bbox_detr, threshold)\n",
        "    #cv2.imwrite(detr_path_t, detr_visualized_cam_threshold)\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.8777, grad_fn=<AddBackward0>)]\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/root/.cache/torch/hub/facebookresearch_detr_main/models/position_encoding.py:41: UserWarning: __floordiv__ is deprecated, and its behavior will change in a future version of pytorch. It currently rounds toward 0 (like the 'trunc' function NOT 'floor'). This results in incorrect rounding for negative values. To keep the current behavior, use torch.div(a, b, rounding_mode='trunc'), or for actual floor division, use torch.div(a, b, rounding_mode='floor').\n",
            "  dim_t = self.temperature ** (2 * (dim_t // 2) / self.num_pos_feats)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.2641, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.2817, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.0671, grad_fn=<AddBackward0>), tensor(10.0423, grad_fn=<AddBackward0>), tensor(8.2343, grad_fn=<AddBackward0>), tensor(8.4077, grad_fn=<AddBackward0>), tensor(11.2500, grad_fn=<AddBackward0>), tensor(8.8875, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(8.7079, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.0867, grad_fn=<AddBackward0>), tensor(10.6647, grad_fn=<AddBackward0>), tensor(9.4661, grad_fn=<AddBackward0>), tensor(8.3907, grad_fn=<AddBackward0>), tensor(11.3320, grad_fn=<AddBackward0>), tensor(7.9635, grad_fn=<AddBackward0>), tensor(10.3141, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(10.3187, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.1791, grad_fn=<AddBackward0>), tensor(13.1549, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.7985, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.4400, grad_fn=<AddBackward0>), tensor(8.5124, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.5693, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(6.2904, grad_fn=<AddBackward0>), tensor(6.4458, grad_fn=<AddBackward0>), tensor(9.4051, grad_fn=<AddBackward0>), tensor(6.3720, grad_fn=<AddBackward0>), tensor(7.9595, grad_fn=<AddBackward0>), tensor(10.3441, grad_fn=<AddBackward0>), tensor(7.6483, grad_fn=<AddBackward0>), tensor(6.8112, grad_fn=<AddBackward0>), tensor(9.3623, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(10.5806, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.1664, grad_fn=<AddBackward0>), tensor(5.9601, grad_fn=<AddBackward0>), tensor(6.9254, grad_fn=<AddBackward0>), tensor(8.0281, grad_fn=<AddBackward0>), tensor(6.0639, grad_fn=<AddBackward0>), tensor(8.3996, grad_fn=<AddBackward0>), tensor(7.6437, grad_fn=<AddBackward0>), tensor(6.5000, grad_fn=<AddBackward0>), tensor(8.7214, grad_fn=<AddBackward0>), tensor(7.4593, grad_fn=<AddBackward0>), tensor(8.0720, grad_fn=<AddBackward0>), tensor(8.2851, grad_fn=<AddBackward0>), tensor(6.7956, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(11.8977, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(7.9916, grad_fn=<AddBackward0>), tensor(12.2227, grad_fn=<AddBackward0>), tensor(8.8902, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.9004, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.0008, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.6022, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.2536, grad_fn=<AddBackward0>), tensor(10.2597, grad_fn=<AddBackward0>), tensor(10.8414, grad_fn=<AddBackward0>), tensor(8.4203, grad_fn=<AddBackward0>), tensor(10.9414, grad_fn=<AddBackward0>), tensor(10.3100, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.2895, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.2339, grad_fn=<AddBackward0>), tensor(9.7704, grad_fn=<AddBackward0>), tensor(10.7959, grad_fn=<AddBackward0>), tensor(9.3671, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.5915, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.8617, grad_fn=<AddBackward0>), tensor(12.9043, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.8770, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.0523, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.7283, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.3215, grad_fn=<AddBackward0>), tensor(10.6083, grad_fn=<AddBackward0>), tensor(10.2522, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.1183, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(7.8299, grad_fn=<AddBackward0>), tensor(10.7542, grad_fn=<AddBackward0>), tensor(12.0724, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.3833, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.3926, grad_fn=<AddBackward0>), tensor(13.0534, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(7.8346, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(13.6772, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.6307, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.7510, grad_fn=<AddBackward0>), tensor(10.6105, grad_fn=<AddBackward0>), tensor(8.9222, grad_fn=<AddBackward0>), tensor(10.3857, grad_fn=<AddBackward0>), tensor(8.9800, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.3451, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.7194, grad_fn=<AddBackward0>), tensor(9.7172, grad_fn=<AddBackward0>), tensor(9.3925, grad_fn=<AddBackward0>), tensor(7.9633, grad_fn=<AddBackward0>), tensor(10.6478, grad_fn=<AddBackward0>), tensor(10.4517, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.1242, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(9.2688, grad_fn=<AddBackward0>), tensor(10.9516, grad_fn=<AddBackward0>), tensor(11.3875, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.8626, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.7770, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.2652, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.7418, grad_fn=<AddBackward0>), tensor(9.2782, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.3589, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.3027, grad_fn=<AddBackward0>), tensor(9.9220, grad_fn=<AddBackward0>), tensor(7.1019, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.4498, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(8.6276, grad_fn=<AddBackward0>), tensor(12.1775, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(4.6841, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.4713, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(6.2599, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(10.6543, grad_fn=<AddBackward0>), tensor(11.9787, grad_fn=<AddBackward0>), tensor(12.5092, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-2.6965, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.0165, grad_fn=<AddBackward0>), tensor(11.5635, grad_fn=<AddBackward0>), tensor(10.8983, grad_fn=<AddBackward0>), tensor(9.7250, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.7369, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.7170, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(5.6064, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(12.6879, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-2.8543, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.6150, grad_fn=<AddBackward0>), tensor(12.4657, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(-7.7046, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.8684, grad_fn=<AddBackward0>), tensor(11.7103, grad_fn=<AddBackward0>), tensor(11.2662, grad_fn=<AddBackward0>)]\n",
            "<class 'tuple'>\n",
            "The max logit value is [tensor(3.9465, grad_fn=<AddBackward0>)]\n",
            "<class 'dict'>\n",
            "The max logit value is [tensor(11.1772, grad_fn=<AddBackward0>), tensor(12.4448, grad_fn=<AddBackward0>)]\n"
          ]
        }
      ]
    }
  ]
}