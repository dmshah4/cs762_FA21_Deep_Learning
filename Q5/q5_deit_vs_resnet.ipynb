{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "q5-deit_vs_resnet.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "bf23f86531834a36bbfc0b0230ef3b1f": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_e794efdddd5d4d0dab60042a97346fd5",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_26fbd71e7e904c9ebb4743a4f0739f89",
              "IPY_MODEL_0de2140e0f374eaca40b895dd70c15e0",
              "IPY_MODEL_c6384d83efca40c3a24076153c924f5b"
            ]
          }
        },
        "e794efdddd5d4d0dab60042a97346fd5": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "26fbd71e7e904c9ebb4743a4f0739f89": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_a19f59c9c5e542d0a8b8fa67290b8c30",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_07c01ae42fa14a0eb38e32f92abc8f90"
          }
        },
        "0de2140e0f374eaca40b895dd70c15e0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_dc6d5ca9ef7a40578694f539a263c8ef",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 102530333,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 102530333,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9468475ae0d34bec8aaf7d3d49a35c3b"
          }
        },
        "c6384d83efca40c3a24076153c924f5b": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_6dc2b958ad0240c8bd7b7ba4671a4bb7",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 97.8M/97.8M [00:00&lt;00:00, 132MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_cd726da1d4cc47c4b45d4663bf22c1d1"
          }
        },
        "a19f59c9c5e542d0a8b8fa67290b8c30": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "07c01ae42fa14a0eb38e32f92abc8f90": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "dc6d5ca9ef7a40578694f539a263c8ef": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9468475ae0d34bec8aaf7d3d49a35c3b": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "6dc2b958ad0240c8bd7b7ba4671a4bb7": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "cd726da1d4cc47c4b45d4663bf22c1d1": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "3752d5f22542498cadd40115c2c1479d": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_50113750314f4da59c87f13e23425b7c",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_a990f21d71a54fcdbe6725475df44c28",
              "IPY_MODEL_ac11e88feceb4bf3982a08db5abc5e79",
              "IPY_MODEL_ccda6d41b4714f7f96e63c748e69c4be"
            ]
          }
        },
        "50113750314f4da59c87f13e23425b7c": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "a990f21d71a54fcdbe6725475df44c28": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_c978d51e85784e82a4d5fc47dd4535d0",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": "100%",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_d902ccfebdcd4659964c8c44f3659af3"
          }
        },
        "ac11e88feceb4bf3982a08db5abc5e79": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "FloatProgressModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_423e491cc8c740689c389ce13817b9b9",
            "_dom_classes": [],
            "description": "",
            "_model_name": "FloatProgressModel",
            "bar_style": "success",
            "max": 22917895,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 22917895,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b0259593ae0049329152662c3aea54ad"
          }
        },
        "ccda6d41b4714f7f96e63c748e69c4be": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_ab6b81f463d9462db646f74fba28a4d6",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "​",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 21.9M/21.9M [00:02&lt;00:00, 12.7MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_b7c817cf45364108881e70e0fb4617eb"
          }
        },
        "c978d51e85784e82a4d5fc47dd4535d0": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "d902ccfebdcd4659964c8c44f3659af3": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "423e491cc8c740689c389ce13817b9b9": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b0259593ae0049329152662c3aea54ad": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "ab6b81f463d9462db646f74fba28a4d6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "model_module_version": "1.5.0",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "b7c817cf45364108881e70e0fb4617eb": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "model_module_version": "1.2.0",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "5zzzudyubDoT",
        "outputId": "e1b02764-4513-440c-eaf2-f893c201e7f3"
      },
      "source": [
        "# !pip install grad-cam\n",
        "!pip install ttach\n",
        "!pip install timm"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: ttach in /usr/local/lib/python3.7/dist-packages (0.0.3)\n",
            "Requirement already satisfied: timm in /usr/local/lib/python3.7/dist-packages (0.4.12)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.7/dist-packages (from timm) (0.11.1+cu111)\n",
            "Requirement already satisfied: torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from timm) (1.10.0+cu111)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch>=1.4->timm) (3.10.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (7.1.2)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.7/dist-packages (from torchvision->timm) (1.19.5)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "hAN7HyEFDpR3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 577
        },
        "outputId": "e2450dda-e19b-4b2c-e144-59f244ac8df0"
      },
      "source": [
        "!git clone https://github.com/vigneshuw/pytorch-grad-cam.git\n",
        "!pip install pytorch-grad-cam/"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "fatal: destination path 'pytorch-grad-cam' already exists and is not an empty directory.\n",
            "Processing ./pytorch-grad-cam\n",
            "\u001b[33m  DEPRECATION: A future pip version will change local packages to be built in-place without first copying to a temporary directory. We recommend you use --use-feature=in-tree-build to test your packages with this new behavior before it becomes the default.\n",
            "   pip 21.3 will remove support for this functionality. You can find discussion regarding this at https://github.com/pypa/pip/issues/7555.\u001b[0m\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "    Preparing wheel metadata ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: tqdm>=4.42 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (4.62.3)\n",
            "Requirement already satisfied: ttach>=0.0.3 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (0.0.3)\n",
            "Requirement already satisfied: opencv-python>=4.5torch>=1.4 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (4.5.4.60)\n",
            "Requirement already satisfied: torchvision>=0.5 in /usr/local/lib/python3.7/dist-packages (from grad-cam==1.3.2) (0.11.1+cu111)\n",
            "Requirement already satisfied: numpy>=1.14.5 in /usr/local/lib/python3.7/dist-packages (from opencv-python>=4.5torch>=1.4->grad-cam==1.3.2) (1.19.5)\n",
            "Requirement already satisfied: torch==1.10.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->grad-cam==1.3.2) (1.10.0+cu111)\n",
            "Requirement already satisfied: pillow!=8.3.0,>=5.3.0 in /usr/local/lib/python3.7/dist-packages (from torchvision>=0.5->grad-cam==1.3.2) (7.1.2)\n",
            "Requirement already satisfied: typing-extensions in /usr/local/lib/python3.7/dist-packages (from torch==1.10.0->torchvision>=0.5->grad-cam==1.3.2) (3.10.0.2)\n",
            "Building wheels for collected packages: grad-cam\n",
            "  Building wheel for grad-cam (PEP 517) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for grad-cam: filename=grad_cam-1.3.2-py3-none-any.whl size=20658 sha256=ff9bd40a7f275acd6c7a65236278e25628f0000695efb78e69fbae8fa26cc78b\n",
            "  Stored in directory: /root/.cache/pip/wheels/cd/cb/7e/31a3553b353ebe761ab45b4267c016ff23c49ba5acd801a813\n",
            "Successfully built grad-cam\n",
            "Installing collected packages: grad-cam\n",
            "  Attempting uninstall: grad-cam\n",
            "    Found existing installation: grad-cam 1.3.2\n",
            "    Uninstalling grad-cam-1.3.2:\n",
            "      Successfully uninstalled grad-cam-1.3.2\n",
            "Successfully installed grad-cam-1.3.2\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.colab-display-data+json": {
              "pip_warning": {
                "packages": [
                  "pytorch_grad_cam"
                ]
              }
            }
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3vhXncVRZOzw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "702d43f7-3d3c-4bf1-ff72-a4c6824878e8"
      },
      "source": [
        "# Google Collab Requirements / Checks\n",
        "from google.colab.patches import cv2_imshow # Change this before running elsewhere!!\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "# Imports\n",
        "import os\n",
        "import cv2\n",
        "import glob\n",
        "import gc\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import copy\n",
        "\n",
        "from PIL import Image\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torchvision.models import resnet50\n",
        "from torchsummary import summary\n",
        "\n",
        "from pytorch_grad_cam import GradCAM\n",
        "from pytorch_grad_cam.utils.image import show_cam_on_image"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BtRudh4lZU-H",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2b9b61b1-2329-4f86-f674-539254757312"
      },
      "source": [
        "# Load manual ms coco dataset and bounding boxes\n",
        "# Varun's Directory\n",
        "dir_uri = '/content/gdrive/MyDrive/University of Wisconsin-Madison/2021 Fall/cs762'\n",
        "# Devesh's Directory\n",
        "# dir_uri = '/content/gdrive/MyDrive'\n",
        "\n",
        "proj_dir = dir_uri + '/CS762_Deep_Learning_Project'\n",
        "#df = pd.read_excel( proj_dir + '/fullBBOX.xlsx')\n",
        "\n",
        "dataset_folder = dir_uri + '/CS762_Deep_Learning_Project/datasets/Manual_Q5_Shuffled' \n",
        "image_names = []\n",
        "for filename in os.listdir(dataset_folder):\n",
        "  image_names.append(filename)\n",
        "\n",
        "image_names.sort(key=lambda x: 10 * int(x.split('-')[0]) + int(x.split('-')[2].split('.')[0]))\n",
        "print(image_names)\n",
        "\n",
        "# Load iamge net categories\n",
        "imageNet_labels_path = dir_uri + \"/CS762_Deep_Learning_Project/datasets/imagenet_classes.txt\"\n",
        "# Read the categories\n",
        "with open(imageNet_labels_path, \"r\") as f:\n",
        "    categories = [s.strip() for s in f.readlines()]\n",
        "\n",
        "# Load bbox's\n",
        "#bbox_path = proj_dir + '/fullBBOX.xlsx'\n",
        "#bbox_df = pd.read_excel(bbox_path)\n",
        "#all_boxes = bbox_df.to_numpy()\n",
        "\n",
        "output_file = proj_dir + '/Q5/q5_output_resnet_deit.csv'\n",
        "headers = ['image_name', 'threshold', 'resnet label', 'resnet accuracy', 'resnet whole image %', 'deit label', 'deit accuracy', 'deit whole image %']"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "['0-person-0.jpg', '0-person-1.jpg', '0-person-2.jpg', '0-person-3.jpg', '1-person-0.jpg', '1-person-1.jpg', '1-person-2.jpg', '1-person-3.jpg', '2-cat-0.jpeg', '2-cat-1.jpeg', '2-cat-2.jpeg', '2-cat-3.jpeg', '3-cat-0.jpeg', '3-cat-1.jpeg', '3-cat-2.jpeg', '3-cat-3.jpeg', '4-dog-0.jpeg', '4-dog-1.jpeg', '4-dog-2.jpeg', '4-dog-3.jpeg', '5-dog-0.png', '5-dog-1.png', '5-dog-2.png', '5-dog-3.png', '6-bird-0.jpeg', '6-bird-1.jpeg', '6-bird-2.jpeg', '6-bird-3.jpeg', '7-bird-0.jpeg', '7-bird-1.jpeg', '7-bird-2.jpeg', '7-bird-3.jpeg']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "vzZ-wNVcZW82"
      },
      "source": [
        "preprocess = transforms.Compose([\n",
        "    transforms.Resize( (224, 224) ),\n",
        "    transforms.ToTensor(),\n",
        "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225]),\n",
        "])\n",
        "\n",
        "def reshape_transform(tensor, height=14, width=14):\n",
        "    result = tensor[:, 1:, :].reshape(tensor.size(0),\n",
        "                                      height, width, tensor.size(2))\n",
        "\n",
        "    # Bring the channels to the first dimension,\n",
        "    # like in CNNs.\n",
        "    result = result.transpose(2, 3).transpose(1, 2)\n",
        "    return result\n",
        "\n",
        "\n",
        "# bbox given as one row from our fullBBox.xlsx file\n",
        "def eval_metric_percent_above(grayscale_cam, bbox, threshold):\n",
        "\n",
        "    # find area of bbox\n",
        "    topLeft_w = int(bbox[2])\n",
        "    topLeft_h = int(bbox[3])\n",
        "    botRight_w = int(bbox[6])\n",
        "    botRight_h = int(bbox[7])\n",
        "    width_bbox = abs(botRight_w - topLeft_w)\n",
        "    height_bbox = abs(topLeft_h - botRight_h)\n",
        "    area = width_bbox * height_bbox\n",
        "\n",
        "    # percent above threshold whole image\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "    count_above_threshold = above_threshold.sum()\n",
        "    total_count = grayscale_cam.shape[0] * grayscale_cam.shape[1]\n",
        "    percent_above_threshold_whole_image = (count_above_threshold / total_count) * 100\n",
        "\n",
        "    # percent above threshold in bbox\n",
        "    bbox_grayscale_cam = grayscale_cam[topLeft_h:botRight_h, topLeft_w:botRight_w]\n",
        "    bbox_above_threshold = (bbox_grayscale_cam > threshold)\n",
        "    bbox_count_above_threshold = bbox_above_threshold.sum()\n",
        "    bbox_total_count = bbox_grayscale_cam.shape[0] * bbox_grayscale_cam.shape[1]\n",
        "    percent_above_threshold_in_bbox = (bbox_count_above_threshold / bbox_total_count) * 100\n",
        "\n",
        "    # percent above threshold outside bbox\n",
        "    outside_bbox_cam = copy.deepcopy(grayscale_cam)\n",
        "    outside_bbox_cam[topLeft_h:botRight_h, topLeft_w:botRight_w] = np.zeros((height_bbox, width_bbox))\n",
        "    outside_bbox_above_threshold = (outside_bbox_cam > threshold)\n",
        "    outside_bbox_count_above_threshold = outside_bbox_above_threshold.sum()\n",
        "    outside_bbox_total_count = total_count - bbox_total_count\n",
        "    percent_above_threshold_outside_bbox = (outside_bbox_count_above_threshold / outside_bbox_total_count) * 100\n",
        "\n",
        "    # return\n",
        "    return percent_above_threshold_whole_image, percent_above_threshold_in_bbox, percent_above_threshold_outside_bbox\n",
        "\n",
        "\n",
        "def eval_metric_percent_above_simple(grayscale_cam, threshold):\n",
        "    # percent above threshold whole image\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "    count_above_threshold = above_threshold.sum()\n",
        "    total_count = grayscale_cam.shape[0] * grayscale_cam.shape[1]\n",
        "    percent_above_threshold_whole_image = (count_above_threshold / total_count) * 100\n",
        "\n",
        "    # return\n",
        "    return percent_above_threshold_whole_image\n",
        "\n",
        "\n",
        "def resizeBbox(orig_bbox, orig_img_size, new_img_size):\n",
        "    orig_h = orig_img_size[0]\n",
        "    orig_w = orig_img_size[1]\n",
        "    new_h = new_img_size[0]\n",
        "    new_w = new_img_size[1]\n",
        "\n",
        "    h_ratio = new_h / orig_h\n",
        "    w_ratio = new_w / orig_w\n",
        "\n",
        "    new_bbox = copy.deepcopy(orig_bbox)\n",
        "    new_bbox[2] = orig_bbox[2] * w_ratio\n",
        "    new_bbox[3] = orig_bbox[3] * h_ratio\n",
        "    new_bbox[4] = orig_bbox[4] * w_ratio\n",
        "    new_bbox[5] = orig_bbox[5] * h_ratio\n",
        "    new_bbox[6] = orig_bbox[6] * w_ratio\n",
        "    new_bbox[7] = orig_bbox[7] * h_ratio\n",
        "    new_bbox[8] = orig_bbox[8] * w_ratio\n",
        "    new_bbox[9] = orig_bbox[9] * h_ratio\n",
        "\n",
        "    return new_bbox"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQVqAqjp315F"
      },
      "source": [
        "preprocess_eval = transforms.Compose([\n",
        "    transforms.Resize( (224, 224) ),\n",
        "])\n",
        "\n",
        "def show_cam_on_image_updated(img: np.ndarray,\n",
        "                      mask: np.ndarray,\n",
        "                      use_rgb: bool = False,\n",
        "                      colormap: int = cv2.COLORMAP_JET) -> np.ndarray:\n",
        "    \"\"\" This function overlays the cam mask on the image as an heatmap.\n",
        "    By default the heatmap is in BGR format.\n",
        "    :param img: The base image in RGB or BGR format.\n",
        "    :param mask: The cam mask.\n",
        "    :param use_rgb: Whether to use an RGB or BGR heatmap, this should be set to True if 'img' is in RGB format.\n",
        "    :param colormap: The OpenCV colormap to be used.\n",
        "    :returns: The default image with the cam overlay.\n",
        "    \"\"\"\n",
        "    heatmap = cv2.applyColorMap(np.uint8(255 * mask), colormap)\n",
        "    if use_rgb:\n",
        "        heatmap = cv2.cvtColor(heatmap, cv2.COLOR_BGR2RGB)\n",
        "    heatmap = np.float32(heatmap) / 255\n",
        "\n",
        "    if np.max(img) > 1:\n",
        "        raise Exception(\n",
        "            \"The input image should np.float32 in the range [0, 1]\")\n",
        "\n",
        "    cam = heatmap + img\n",
        "    cam = cam / np.max(cam)\n",
        "    return heatmap, np.uint8(255 * cam)\n",
        "\n",
        "\n",
        "def visualize_eval_metric(rgb_image, grayscale_cam, heatmap, bbox, threshold):\n",
        "    red_color = (0, 0, 255)\n",
        "    thickness = 2\n",
        "    start_point = (int(bbox[2]), int(bbox[3]))\n",
        "    end_point = (int(bbox[6]), int(bbox[7]))\n",
        "\n",
        "    above_threshold = (grayscale_cam > threshold)\n",
        "\n",
        "    for i in range(grayscale_cam.shape[0]):\n",
        "      for j in range(grayscale_cam.shape[1]):\n",
        "        if(above_threshold[i, j] == False):\n",
        "          heatmap[i,j,0] = 0\n",
        "          heatmap[i,j,1] = 0\n",
        "          heatmap[i,j,2] = 0\n",
        "\n",
        "    cam = heatmap + rgb_image\n",
        "    cam = cam / np.max(cam)\n",
        "    cam = np.uint8(255 * cam)\n",
        "    cam = cv2.rectangle(cam, start_point, end_point, red_color, thickness)\n",
        "    return cam"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 133,
          "referenced_widgets": [
            "bf23f86531834a36bbfc0b0230ef3b1f",
            "e794efdddd5d4d0dab60042a97346fd5",
            "26fbd71e7e904c9ebb4743a4f0739f89",
            "0de2140e0f374eaca40b895dd70c15e0",
            "c6384d83efca40c3a24076153c924f5b",
            "a19f59c9c5e542d0a8b8fa67290b8c30",
            "07c01ae42fa14a0eb38e32f92abc8f90",
            "dc6d5ca9ef7a40578694f539a263c8ef",
            "9468475ae0d34bec8aaf7d3d49a35c3b",
            "6dc2b958ad0240c8bd7b7ba4671a4bb7",
            "cd726da1d4cc47c4b45d4663bf22c1d1",
            "3752d5f22542498cadd40115c2c1479d",
            "50113750314f4da59c87f13e23425b7c",
            "a990f21d71a54fcdbe6725475df44c28",
            "ac11e88feceb4bf3982a08db5abc5e79",
            "ccda6d41b4714f7f96e63c748e69c4be",
            "c978d51e85784e82a4d5fc47dd4535d0",
            "d902ccfebdcd4659964c8c44f3659af3",
            "423e491cc8c740689c389ce13817b9b9",
            "b0259593ae0049329152662c3aea54ad",
            "ab6b81f463d9462db646f74fba28a4d6",
            "b7c817cf45364108881e70e0fb4617eb"
          ]
        },
        "id": "Gye9G6bYcmPU",
        "outputId": "3c275db6-0214-4938-be74-a06723fcc6f7"
      },
      "source": [
        "resnet_model = resnet50(pretrained=True)\n",
        "resnet_model.eval();\n",
        "\n",
        "deit_model = torch.hub.load('facebookresearch/deit:main',\n",
        "                          'deit_tiny_patch16_224', pretrained=True)\n",
        "deit_model.eval();"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet50-0676ba61.pth\" to /root/.cache/torch/hub/checkpoints/resnet50-0676ba61.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "bf23f86531834a36bbfc0b0230ef3b1f",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/97.8M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Downloading: \"https://github.com/facebookresearch/deit/archive/main.zip\" to /root/.cache/torch/hub/main.zip\n",
            "Downloading: \"https://dl.fbaipublicfiles.com/deit/deit_tiny_patch16_224-a1311bcf.pth\" to /root/.cache/torch/hub/checkpoints/deit_tiny_patch16_224-a1311bcf.pth\n"
          ]
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3752d5f22542498cadd40115c2c1479d",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "  0%|          | 0.00/21.9M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {}
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "_JLGhFsReLy5"
      },
      "source": [
        "resnet_target_layers = [resnet_model.layer4[-1]]\n",
        "deit_target_layers = [deit_model.blocks[-1].norm1]\n",
        "\n",
        "target_category = None\n",
        "\n",
        "#resnet_input_image_shape = (224, 224, 3)\n",
        "#deit_input_image_shape = (224, 224, 3)\n",
        "\n",
        "threshold_options = [0.7, 0.9]"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "4M5Uzx3Mc_RM",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "78dd245e-b844-41ae-e0ba-160fe5f686d0"
      },
      "source": [
        "output_data = []\n",
        "\n",
        "for threshold in threshold_options:\n",
        "  for i in range( len( image_names ) ):\n",
        "    # Create input\n",
        "    test_img = Image.open( dataset_folder + '/' + image_names[i] ).convert('RGB')\n",
        "    input_tensor = preprocess(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "    #orig_bbox = all_boxes[i]\n",
        "    orig_img_shape = np.array(test_img).shape\n",
        "\n",
        "    # Create target lable\n",
        "    target_name = image_names[i].split( '-' )[1]\n",
        "    target_name = target_name.split( '.' )[0]\n",
        "\n",
        "    # Set target category\n",
        "    # target_category = [index for index, class_instance in enumerate(categories) if class_instance == target_name]\n",
        "\n",
        "    # If target category is empty (ie label does not exist in image net, skip this image):\n",
        "    #if ( len( target_category ) == 0 ):\n",
        "    #  continue\n",
        "\n",
        "    print( image_names[i] )\n",
        "\n",
        "    # RESNET\n",
        "\n",
        "    with torch.no_grad():\n",
        "      output = resnet_model(input_batch)\n",
        "\n",
        "    # The output has unnormalized scores. To get probabilities, you can run a softmax on it.\n",
        "    probabilities = torch.nn.functional.softmax(output[0], dim=0)\n",
        "\n",
        "    # Get top category\n",
        "    top5_prob, top5_catid = torch.topk(probabilities, 5)\n",
        "    resnet_predicted_label = categories[top5_catid[0]]\n",
        "    resnet_predicted_label_prob = top5_prob[0].item()\n",
        "\n",
        "    resnet_cam = GradCAM(model=resnet_model, target_layers=resnet_target_layers)\n",
        "    resnet_grayscale_cam = resnet_cam(input_tensor=input_batch, target_category=target_category)\n",
        "    resnet_grayscale_cam = resnet_grayscale_cam[0, :]\n",
        "    \n",
        "    # resize bbox and calculate metrics\n",
        "    #new_bbox_resnet = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (resnet_input_image_shape[0], resnet_input_image_shape[1]))\n",
        "    #resnet_whole_img, resnet_in_bbox, resnet_out_bbox = eval_metric_percent_above(resnet_grayscale_cam, new_bbox_resnet, threshold)\n",
        "    resnet_whole_img = eval_metric_percent_above_simple(resnet_grayscale_cam, threshold)\n",
        "\n",
        "    # DEIT\n",
        "\n",
        "    # compute the predictions\n",
        "    with torch.no_grad():\n",
        "      output = deit_model(input_batch)\n",
        "\n",
        "    # and convert them into probabilities\n",
        "    scores = torch.nn.functional.softmax(output, dim=-1)[0]\n",
        "\n",
        "    # finally get the index of the prediction with highest score\n",
        "    topk_scores, topk_label = torch.topk(scores, k=5, dim=-1)\n",
        "    deit_predicted_label = categories[topk_label[0]]\n",
        "    deit_predicted_label_prob = topk_scores[0].item()\n",
        "\n",
        "    deit_cam = GradCAM(model=deit_model, target_layers=deit_target_layers, reshape_transform=reshape_transform)\n",
        "    deit_grayscale_cam = deit_cam(input_tensor=input_batch, target_category=target_category)\n",
        "    deit_grayscale_cam = deit_grayscale_cam[0, :]\n",
        "\n",
        "    # resize bbox and calculate metrics\n",
        "    #new_bbox_deit = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (deit_input_image_shape[0], deit_input_image_shape[1]))\n",
        "    #deit_whole_img, deit_in_bbox, deit_out_bbox = eval_metric_percent_above(deit_grayscale_cam, new_bbox_deit, threshold)\n",
        "    deit_whole_img = eval_metric_percent_above_simple(deit_grayscale_cam, threshold)\n",
        "\n",
        "    output_data.append( [image_names[i], threshold, resnet_predicted_label, resnet_predicted_label_prob, resnet_whole_img, deit_predicted_label, deit_predicted_label_prob, deit_whole_img] )\n",
        "\n",
        "    # Delete garbage for ram\n",
        "    gc.collect()"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "0-person-0.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.5150, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.4606, grad_fn=<AddBackward0>)]\n",
            "0-person-1.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1059, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.8955, grad_fn=<AddBackward0>)]\n",
            "0-person-2.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3287, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.4502, grad_fn=<AddBackward0>)]\n",
            "0-person-3.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.9703, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.1024, grad_fn=<AddBackward0>)]\n",
            "1-person-0.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.8806, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4446, grad_fn=<AddBackward0>)]\n",
            "1-person-1.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8645, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(5.6813, grad_fn=<AddBackward0>)]\n",
            "1-person-2.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3493, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1559, grad_fn=<AddBackward0>)]\n",
            "1-person-3.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.8818, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.5040, grad_fn=<AddBackward0>)]\n",
            "2-cat-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.2345, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1301, grad_fn=<AddBackward0>)]\n",
            "2-cat-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.1727, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.7390, grad_fn=<AddBackward0>)]\n",
            "2-cat-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.6507, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1020, grad_fn=<AddBackward0>)]\n",
            "2-cat-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(15.7635, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1450, grad_fn=<AddBackward0>)]\n",
            "3-cat-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.8316, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.9851, grad_fn=<AddBackward0>)]\n",
            "3-cat-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.2334, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8992, grad_fn=<AddBackward0>)]\n",
            "3-cat-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(11.7343, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8154, grad_fn=<AddBackward0>)]\n",
            "3-cat-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.1569, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.9560, grad_fn=<AddBackward0>)]\n",
            "4-dog-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(14.9715, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.2353, grad_fn=<AddBackward0>)]\n",
            "4-dog-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(14.3774, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.9772, grad_fn=<AddBackward0>)]\n",
            "4-dog-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.8162, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.1912, grad_fn=<AddBackward0>)]\n",
            "4-dog-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.9064, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.4217, grad_fn=<AddBackward0>)]\n",
            "5-dog-0.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.4066, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3834, grad_fn=<AddBackward0>)]\n",
            "5-dog-1.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.4811, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4301, grad_fn=<AddBackward0>)]\n",
            "5-dog-2.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(25.5416, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.8440, grad_fn=<AddBackward0>)]\n",
            "5-dog-3.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.7867, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.6555, grad_fn=<AddBackward0>)]\n",
            "6-bird-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.7803, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.5043, grad_fn=<AddBackward0>)]\n",
            "6-bird-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.2640, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.6556, grad_fn=<AddBackward0>)]\n",
            "6-bird-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(15.2330, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4346, grad_fn=<AddBackward0>)]\n",
            "6-bird-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(18.7454, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3832, grad_fn=<AddBackward0>)]\n",
            "7-bird-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.3815, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.0512, grad_fn=<AddBackward0>)]\n",
            "7-bird-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.9330, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8044, grad_fn=<AddBackward0>)]\n",
            "7-bird-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(11.5024, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.5066, grad_fn=<AddBackward0>)]\n",
            "7-bird-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.3733, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.6551, grad_fn=<AddBackward0>)]\n",
            "0-person-0.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.5150, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.4606, grad_fn=<AddBackward0>)]\n",
            "0-person-1.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1059, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.8955, grad_fn=<AddBackward0>)]\n",
            "0-person-2.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3287, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.4502, grad_fn=<AddBackward0>)]\n",
            "0-person-3.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.9703, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.1024, grad_fn=<AddBackward0>)]\n",
            "1-person-0.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.8806, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4446, grad_fn=<AddBackward0>)]\n",
            "1-person-1.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8645, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(5.6813, grad_fn=<AddBackward0>)]\n",
            "1-person-2.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3493, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1559, grad_fn=<AddBackward0>)]\n",
            "1-person-3.jpg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.8818, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.5040, grad_fn=<AddBackward0>)]\n",
            "2-cat-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.2345, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1301, grad_fn=<AddBackward0>)]\n",
            "2-cat-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.1727, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.7390, grad_fn=<AddBackward0>)]\n",
            "2-cat-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.6507, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1020, grad_fn=<AddBackward0>)]\n",
            "2-cat-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(15.7635, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1450, grad_fn=<AddBackward0>)]\n",
            "3-cat-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.8316, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.9851, grad_fn=<AddBackward0>)]\n",
            "3-cat-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.2334, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8992, grad_fn=<AddBackward0>)]\n",
            "3-cat-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(11.7343, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8154, grad_fn=<AddBackward0>)]\n",
            "3-cat-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.1569, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.9560, grad_fn=<AddBackward0>)]\n",
            "4-dog-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(14.9715, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.2353, grad_fn=<AddBackward0>)]\n",
            "4-dog-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(14.3774, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.9772, grad_fn=<AddBackward0>)]\n",
            "4-dog-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.8162, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.1912, grad_fn=<AddBackward0>)]\n",
            "4-dog-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.9064, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.4217, grad_fn=<AddBackward0>)]\n",
            "5-dog-0.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.4066, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3834, grad_fn=<AddBackward0>)]\n",
            "5-dog-1.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.4811, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4301, grad_fn=<AddBackward0>)]\n",
            "5-dog-2.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(25.5416, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.8440, grad_fn=<AddBackward0>)]\n",
            "5-dog-3.png\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.7867, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.6555, grad_fn=<AddBackward0>)]\n",
            "6-bird-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.7803, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.5043, grad_fn=<AddBackward0>)]\n",
            "6-bird-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.2640, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.6556, grad_fn=<AddBackward0>)]\n",
            "6-bird-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(15.2330, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4346, grad_fn=<AddBackward0>)]\n",
            "6-bird-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(18.7454, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3832, grad_fn=<AddBackward0>)]\n",
            "7-bird-0.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.3815, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.0512, grad_fn=<AddBackward0>)]\n",
            "7-bird-1.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.9330, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8044, grad_fn=<AddBackward0>)]\n",
            "7-bird-2.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(11.5024, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.5066, grad_fn=<AddBackward0>)]\n",
            "7-bird-3.jpeg\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.3733, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.6551, grad_fn=<AddBackward0>)]\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "IDgBUlnOxPAb"
      },
      "source": [
        "df = pd.DataFrame (output_data, columns=headers)\n",
        "df.to_csv(output_file)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MPP9v4vq1ymq"
      },
      "source": [
        "output_images_dir = proj_dir + '/Q5/output_images/'\n",
        "threshold = 0.7\n",
        "generate_vis_analysis = True"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "YiBAMlXC3dN-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ab0ec99d-8c79-42bc-940d-1cb6e6a7bd79"
      },
      "source": [
        "if(generate_vis_analysis):\n",
        "  for i in range( len( image_names ) ):\n",
        "    # Create input\n",
        "    test_img = Image.open( dataset_folder + '/' + image_names[i] ).convert('RGB')\n",
        "    resnet_path = output_images_dir + image_names[i] + '_resnet_whole'\n",
        "    deit_path = output_images_dir + image_names[i] + '_deit_whole'\n",
        "    input_tensor = preprocess(test_img)\n",
        "    input_batch = input_tensor.unsqueeze(0) # create a mini-batch as expected by the model\n",
        "    #orig_bbox = all_boxes[i]\n",
        "    #orig_img_shape = np.array(test_img).shape\n",
        "\n",
        "    rgb_image = np.array(preprocess_eval(test_img)).astype(np.float32)\n",
        "    rgb_image = rgb_image/255\n",
        "\n",
        "    # Resnet\n",
        "    resnet_cam = GradCAM(model=resnet_model, target_layers=resnet_target_layers)\n",
        "    resnet_grayscale_cam = resnet_cam(input_tensor=input_batch, target_category=target_category)\n",
        "    resnet_grayscale_cam = resnet_grayscale_cam[0, :]\n",
        "\n",
        "    resnet_heatmap, resnet_visualization = show_cam_on_image_updated(rgb_image, resnet_grayscale_cam, use_rgb=False)\n",
        "    cv2.imwrite(resnet_path, resnet_visualization)\n",
        "\n",
        "    #new_bbox_resnet = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (resnet_input_image_shape[0], resnet_input_image_shape[1]))\n",
        "    #resnet_visualized_cam_threshold = visualize_eval_metric(rgb_image, resnet_grayscale_cam, resnet_heatmap, new_bbox_resnet, threshold)\n",
        "    #cv2.imwrite(resnet_path_t, resnet_visualized_cam_threshold)\n",
        "\n",
        "    # DEIT\n",
        "    rgb_image = np.array(preprocess_eval(test_img)).astype(np.float32)\n",
        "    rgb_image = rgb_image/255\n",
        "\n",
        "    deit_cam = GradCAM(model=deit_model, target_layers=deit_target_layers, reshape_transform=reshape_transform)\n",
        "    deit_grayscale_cam = deit_cam(input_tensor=input_batch, target_category=target_category)\n",
        "    deit_grayscale_cam = deit_grayscale_cam[0, :]\n",
        "\n",
        "    deit_heatmap, deit_visualization = show_cam_on_image_updated(rgb_image, deit_grayscale_cam, use_rgb=False)\n",
        "    cv2.imwrite(deit_path, deit_visualization)\n",
        "\n",
        "    #new_bbox_deit = resizeBbox(orig_bbox, (orig_img_shape[0], orig_img_shape[1]), (deit_input_image_shape[0], deit_input_image_shape[1]))\n",
        "    #deit_visualized_cam_threshold = visualize_eval_metric(rgb_image, deit_grayscale_cam, deit_heatmap, new_bbox_deit, threshold)\n",
        "    #cv2.imwrite(deit_path_t, deit_visualized_cam_threshold)\n",
        "\n"
      ],
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.5150, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.4606, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1059, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.8955, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3287, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.4502, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.9703, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.1024, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.8806, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4446, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8645, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(5.6813, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3493, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1559, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.8818, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.5040, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.2345, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1301, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.1727, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.7390, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.6507, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1020, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(15.7635, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.1450, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(13.8316, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.9851, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.2334, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8992, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(11.7343, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8154, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.1569, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.9560, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(14.9715, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.2353, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(14.3774, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.9772, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.8162, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.1912, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.9064, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.4217, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.4066, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3834, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.4811, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4301, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(25.5416, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(6.8440, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.7867, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.6555, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(16.7803, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.5043, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(17.2640, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.6556, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(15.2330, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.4346, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(18.7454, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.3832, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.3815, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(9.0512, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(12.9330, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(7.8044, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(11.5024, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.5066, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(10.3733, grad_fn=<AddBackward0>)]\n",
            "<class 'torch.Tensor'>\n",
            "The max logit value is [tensor(8.6551, grad_fn=<AddBackward0>)]\n"
          ]
        }
      ]
    }
  ]
}